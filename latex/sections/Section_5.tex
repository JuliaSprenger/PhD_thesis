\clearpage
\section{Structuring scientific work - Workflow management}
\label{sec:workflows}
The process from data generation to a scientific publication can be usually split into individual steps performing only a subset of the complete processing requirend. The separation of these steps can occur on different levels from very coarse like the separation of the experiment and analysis to very fine where each individual processing step is implemented in a independent script.

Workflow management is a concept to organize the individual steps forming a process. The granularity of the steps to manage highly depend on the complexity of the tasks and the diversity of the processing steps. A common and generic example forming a workflow management system is a queueing system used for cluster computing. Here users submit a number of in the simplest case independent jobs (computing steps) which are then depending on the available resources scheduled and distributed to suitable compute resources. This is a simple example, because the individual processing steps typically do not depend on each other and only the required amount of resources and time needs to be taken into account when organizing the execution.

For scientific projects like the Reach-to-Grasp experiment described in \cref{sec:data} there are dependencies between individual steps of the process from data acquisition to publication (see \cref{fig:scidata_metadata_pipeline,fig:scidata_reachgraspio_diagram}). The workflow management concept has been applied in a number of scientifics fields like genomics or imaging data. In these fields a systematic approach to data processing and analysis is required and feasable, since the they are dealing with large and numerous datasets which exceed manual monitoring or processing power \citep[e.g.][]{Palm_2010}.
For these disciplines, there are a number of platforms and tools available to implement pre \& postprocessing as well as analysis processing steps: Galaxy\footnote{Galaxy, \url{https://galaxyproject.org}, RRID:SCR\_006281} an  open, web-based platform providing bioinformatics tools and services for data intensive genomic research, vistrails\footnote{VisTrails, \url{https://www.vistrails.org}, RRID:SCR\_006261} an open-source scientific workflow and provenance management system that provides support for simulations, data exploration and visualization, Taverna\footnote{Taverna, \url{https://taverna.incubator.apache.org}, RRID:SCR\_004437} a scalable, open source \& domain independent tools for designing and executing workflows,  GenePattern\footnote{GenePattern, \url{http://www.broadinstitute.org/cancer/software/genepattern}, RRID:SCR\_003201} a genomic analysis platform that provides access to hundreds of tools for gene expression analysis, proteomics, SNP analysis, flow cytometry, RNA-seq analysis, and common data processing tasks, Renku\footnote{Renku, \url{https://datascience.ch/renku}} an online software platform for reproducible and collaborative data science including workflow management aspects, Terra\footnote{\url{https://terra.bio/}} a scalable platform  for biomedical research for data analysis and collaboration, Ugene \cite{Okonechnikov_2012} a multiplatform open-source software for molecular biology and snakemake\footnote{Snakemake, url{https://snakemake.readthedocs.io/en/stable/}, RRID:SCR\_003475} a Python based language and execution environment for make-like workflows.

In this section we present the usage of snakemake as a workflow management tool as it is domain independent, slim and easily integrates with Python based projects, e.g. to the Reach-to-Grasp and related projects \cref{sec:data}.

\subsection{Workflow management tools - Snakemake}
Snakemake is a generic workflow management tool derived from the build automation tool Make combined with Python features. It is available as bioconda\footnote{\url{https://anaconda.org/bioconda/snakemake}} and PyPi package\footnote{\url{https://pypi.org/project/snakemake}} with the latest version $5.5.4$ being considered here.

We demonstrate the basic features of snakemake based on two minimal workflow examples. The first one (\cref{code:workflows_simple_snakefile}) generates and copies files in two alternative ways and the second, more complex workflow uses two python scripts (\ref{code:workflows_python_scripts}) to generate data using \software{Neo} and visualize it using \software{Matplotlib} (\cref{code:workflows_python_snakefile,fig:python_demo}).

The description of individual steps of a workflow within snakemake is closely related to Make: A processing step is defined via its input and output files (\cref{code:workflows_simple_snakefile} line 11 and 12). The core of a rule is the instruction how to generate the output files based on the input files. Here snakemake offers multiple options based on direct execution of Python scripts or bash scripting. Bash scripts offer the most flexibility and are maked with the \code{shell} keyword (\cref{code:workflows_simple_snakefile} line 13). Within executed shell command references to the input and output files can be used via Python based reformatting of the command before execution. E.g. in \cref{code:workflows_simple_snakefile} line 13 the filename specified by the input of the rule \code{simple\_copy\_rule} (line 11) is automatically copied to the filename specified by the output of the rule by using \code{\{input\}} and \code{\{output\}} in the shell command. The same concept can be used to formulate snakemake rules in a more flexible fashion. E.g. in \cref{code:workflows_simple_snakefile} a set of flexible rules is introduces, which use an additional wildcard \code{\{filename\}} to be able to generate and copy not only files with filename \code{'file.md'}, but any markdown file. Here, the value of the variable \code{\{filename\}} is only determined during the exectution of the workflow. Therefore the same rule can be used multiple times within a workflow with different wildcard parameters. Hereby the value of the wildcard is determined recursively by the required output file.

The dependencies between snakemake rules are evaluated based on required files. By default snakemake uses the first rule within a \code{Snakefile} as main rule and tries to execute this rule. Alternatively snakemake can be called with an filename as an argument. In this case snakemake attempts to build the requested file based on all available rules matching in and output files of rules and checking the availability of basic input files. For this purpose snakemake generates an acyclic directed graph of rule dependencies (e.g. see \cref{fig:workflows_python_snakefile}) and infers all wildcard parameters from this. In case multiple rules can be used for generation of the same file a rule priority order can be be defined (\cref{code:workflows_simple_snakefile} line 1-3). Snakemake only executes rules and creates or overwrites files if the output files of a rule does not exist or the input files have a more recent modification time stamp the output files. This guarantees, that the output files of a snakemake workflow are always based on the most recent version of input files and at the same time minimizes the computational overhead, since only required or outdated files are generated.

Snakemake rule can be executed in dedicated containerized environments. For Python workflows snakemake supports the build of conda environments on a rule level. For each rule the conda environment can be defined via a \code{yaml} files specifying the required dependencies (see \cref{code:workflows_python_snakefile} snakefile line 16 and 23 and environments). Snakemake builds the environment using conda when requiring it the first time and keeps a reference. In case the \code{yaml} environment definition changes, the environment is automatically regenerated.

\cref{code:workflows_python_snakefile} demonstrates a more complex workflow using generic Python scripts for data generating based no the \software{Neo} package and visualization using the Python \software{Matplotlib} package (\cref{code:workflows_python_scripts}. These scripts are implemented to be used as standalone scripts, executed and provided with arguments from the command line. Additionally, they are not relying on a fixed data format, but support any format available in the \software{Neo} framework. This, in combination with the explicit definition of the required python packages in form of \code{yaml} files make the scripts highly flexible and generic, such that they can be easily reused in different contexts and projects. Furthermore, the snakemake implementation of the workflow keeps the genericity of the code by providing flexibility in the used data format, which is defined via an additional configuration \code{yaml} file, and the usage of the usage of wildcards for flexible handling of filenames. The resulting snakemake workflow as well as the output visualization of the randomly generated data can be seen in \cref{fig:python_demo}.


\begin{codeenv}
\textbf{Snakemake header}
\inputminted[firstline=1, lastline=3, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/simple_demo.snakefile}
\begin{multicols}{2}
\textbf{Simple rules}
\inputminted[firstline=5, lastline=13, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/simple_demo.snakefile}
\columnbreak
\textbf{Flexible rules}
\inputminted[firstline=15, lastline=23, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/simple_demo.snakefile}
\end{multicols}
\caption[Minimal snakemake example workflow]{Minimal snakemake example workflow. The workflow consists of two rules, for generation of a markdown file (.md) and conversion to a text file by plain copy of the content into a file with .txt extension. There are two versions of each rule demonstrating snakemake features at different complexities: The simple version of the rule handles filenames explicitely, whereas the flexible version of the rule is using wildcards to handle filenames. To resolve ambiguities between the two versions of the rules, we define a rule priority order in the first lines of the snakemake file.}
\label[codelisting]{code:workflows_simple_snakefile}
\end{codeenv}


\begin{codeenv}
\begin{multicols}{2}
\textbf{Snakefile}\\
\inputminted[firstline=1, lastline=40, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/python_demo.snakefile}
\columnbreak
\textbf{Environments}\\
\textbf{plotting\_environment.yaml}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{yaml}{figures/workflows/envs/plotting_environment.yaml}
\textbf{data\_generation\_environment.yaml}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{yaml}{figures/workflows/envs/data_generation_environment.yaml}
\textbf{config.yaml}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{yaml}{figures/workflows/config.yaml}
\end{multicols}
\caption[Snakemake example workflow for data generation and plotting]{Snakemake example workflow for data generation and plotting. The workflow consists of three rules, for data generation, data visualization and specification of the all output files of the workflow. The first two rules can be executed in dedicated conda environments, specified via the \code{conda:} directive and shown at the right. The workflow uses a configuration file (Snakefile line 1, \code{config.yaml}), specifying the format for storing \software{Neo} structures. This specification is also used to provide a constraint for wildcards with the name \code{data\_ext}, which resolves ambiguities between the data generation and visualization rule. The rule \code{all}, is by default executed when snakemake is run, it specifies two required output formats of the workflow. For the visualizaion of the workflow diagram when running the \code{all} rule see \ref{fig:python_demo}.}
\label[codelisting]{code:workflows_python_snakefile}
\end{codeenv}

\begin{figure}
%     \begin{multicols}{2}
    \begin{minipage}[t]{0.4\textwidth}
    \textbf{A}\\
    \includesvg[width=\textwidth, pretex=\relscale{0.8}]{figures/workflows/python_demo_escus}
    \end{minipage}
    \begin{minipage}[t]{0.6\textwidth}
%     \columnbreak\\
    \textbf{B}\\
    \includesvg[width=\textwidth]{figures/workflows/data}\\
    \end{minipage}
    %     \end{multicols}
 \caption[Snakemake example workflow for data generation and plotting]{Snakemake example workflow for data generation and plotting. The workflow diagram (left) and result (right). The workflow consists of two rules of which the \code{plot\_data} rule is executed twice with different parameters to generate the final plot in two file formats (\code{ext: svg}, \code{ex:png}, respecively). Different rules are color coded and the rule name is indicated at the top of each node. The frame style (solid/dashed) indicates if this rule needs to be run to generate a final output file. The arrows indicate the dependencies between the rule executions, rules at the top need to be executed first, since they generate output files that are required as input for the subsequent rules executions.}
\label{fig:python_demo}
\end{figure}


\begin{codeenv}
\begin{multicols}{2}
\textbf{Data generation}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{python}{figures/workflows/generate_data.py}
\columnbreak
\textbf{Data visualization}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{python}{figures/workflows/plot_data.py}
\end{multicols}
\caption[Standalone Python scripts used in \cref{code:workflows_python_snakefile}]{Standalone Python scripts used in \cref{code:workflows_python_snakefile}. The two scripts for data generation and visualization contain generic functions, relying on command line parameters to provide the arguments for the function calls (line 21-24 and line 18-21, respectively). The \textbf{data generation} is split into two functions, one for generation of the \software{Neo} structure (\code{generate\_neo\_data}) and one for saving the \software{Neo} structure to disc (\code{save\_neo\_block}). The first function generates a \software{Neo} \code{Block} containing a single \code{AnalogSignal} with randomly generated data (line 4-12). The second function recieves an generic \software{Neo} \code{Block} and saves it in the format specified by the provided filename (line 16-19). If the script is executed from the command line the input parameter \code{filename} is extracted from the command line arguments and both functions are executed consecutively, passing the \software{Neo} \code{Block} from one function the next (line 21-24). The \textbf{data visualization} uses the same concept as the data generation. Here the two internal functions are loading a \software{Neo} block from the specified data source filename (\code{load\_neo\_block}, line 4-7) and visualize the first \code{AnalogSignal} of a given plot, saving the result in a requested filename (code{plot\_analogsignal}, line 9-16). Both functions are called if the script is called from the command line and the two parameters specifying the data location as well as the output plot filename are extracted from the command line arguments.
}
\label[codelisting]{code:workflows_python_scripts}
\end{codeenv}

In addition to the features demonstrated in the example scripts, snakemake integrates well distributed storage concepts, such as Google Cloud Storage\footnote{Google Cloud Storage\url{https://cloud.google.com/storage/}}, DropBox\footnote{DropBox\url{https://www.dropbox.com}} or the secure shell protocol (SSH). Remote file sources are declared in the header of the snakemake file and individual files can be referenced from these sources in the same manner as local files. Besides access to remote files, snakemake also integrates with high-performance compute clusters by supporting common queueing systems such as the slurm workload manager\footnote{slurm, \url{https://slurm.schedmd.com}}. Here, a configuration file can be used to specify the custer job parameters on a rule-level, permitting detailed resource management.

\paragraph{Summary}
We presented the application of basice snakemake workflow features based on two example workflows demonstrating the modularization of a workflow into individual rules and their file-based dependency handling. We highlighted the flexibility of this approach by introducing wildcard based filename handling and explained the snakemake dependency graph. We provide examples of generic standalone Python scripts for seamless integration into snakemake rules and demonstrated advanced configuration features of the workflow via additional configuration files and wildcard constraints. We introduce additional features for integration of remote files and cluster usage.
The presented features make snakemake our tool of choice for the implementation of scientific workflows, as it provides a domain-independent and slim option for workflow definition which integrates well with existing scripted data processing steps.


\todo{inspiration from computer sciences \& industrial software development: Continuous integration \& deployment}



\subsection{Practical application}
Snakemake has been applied in a variety of different fields and projects. Many of the provided examples and tutorials are set in the field of genomics\footnote{\url{https://snakemake.readthedocs.io/en/stable/getting_started/examples.html}}\footnote{\url{https://snakemake.readthedocs.io/en/stable/tutorial/basics.html}} here we present a workflow design in the context of the Vision4Action project, a successor project of Reach-to-Grasp.

\subsubsection{The Vision-For-Action project}
The Vision-For-Action project is building on top of the Reach-to-Grasp project by extending the investigation of motor control only to the interaction of motor and visual activity. For this a modified task is used, involving the continuous integration of visual input and motor control. The experimental hardware is a Real-time visuomotor behavior and electrophysiology recording
(RIVER) setup and consists partially of the same components used in the Reach-to-Grasp experiment. For a detailed description of the recording setup see \cite{De_Haan_2018,De_Haan_2018a}. The Reach-to-Grasp experiment was extended by an eye as well as hand movement control system and a complex task design including the sequential pointing to up to six targets. To the current date, only a single monkey was recorded in the Vision-For-Action setup.

\paragraph{The task}
In the context of the Vision-For-Action experiment the monkey views a horizontal, semi-transparent mirror on which a red dot is projected corresponding to the location of the monkeys hand below the mirror (\cref{fig:river_setup}). The monkey is trained to initialize a task by moving the hand curser into the area of a central, illuminated target. After a waiting period the central target is deactivated and depending on the task type one or multiple of 6 peripheral targets are illuminated. The to recieve a reward, the monkey has to deactivate all illuminated targets by moving the hand curser into the target. Depending on the task also additional targets might be activated when deactivating the previous target. Two classic task types are implemented currently: The the landing task, in which the monkey is presented a sequence of three peripheral targets, which he has to deactivate by staying in the targets for $??$ seconds \todo{how many seconds? scan descriptors and ask fred} leading to direct movements to the target. The drawing task, in which the monkey is presented multiple targets at once and can chose an order and route to deactivate these. In this task type the monkey only briefly needs to touch the target with the hand curser leading to very curved movement trajectories. 
\todo{is this already published somewhere? am I permitted to write this?}


\paragraph{The setup}
The setup consists of 

The arm of the monkey is attached to a Kinarm system\footnote{Kinarm, \url{https://kinarm.com/}}, which can be used to track and interfer with the monkey's movement. 

\begin{sidewaysfigure}
 \includegraphics[width=\textwidth]{figures/workflows/river_setup_hardware}
 \caption[The river setup]{The river setup including schematic of hardware components and signal flows. Depicted are the monkey task setup (top right), the recording system and signal flows (bottom left), the monkey chair and kinarm (top right) and the recording hardware rack (bottom right). Figure from \citet{De_Haan_2018a}.}
 \label{fig:river_setup}
\end{sidewaysfigure}


\subsection{Waves project}
\subsection{Summary \& Guidelines}



\begin{figure}
    \centering
    \includesvg[width=\textwidth, pretex=\relscale{0.7}]{./figures/workflows/rulegraph_colored_escus}
    \caption[Metadata workflow for Vision4Action experiment]{Metadata workflow for Vision4Action experiment.  Metadata are aggregated, data are loaded and combined in multiple workflow steps.}
    \label{fig:demo_visualization}
\end{figure}



\todo{workflow management systems: Renku, https://terra.bio/, ugene, from genomics: galaxy, vistrains, Taverna,  GenePattern...}
\todo{introduce the concept of workflow management, snakemake, Apps,  categories
- general apps
- experiment specific apps}
