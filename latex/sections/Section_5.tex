\clearpage
\section{Structuring scientific work - Workflow management}
\label{sec:workflows}

\todo{Include snakemake demo workflow 1) in manuscript snakemake and 2) in manuscript below}

The process from data generation to a scientific publication can be usually split into individual steps performing only a subset of the complete processing requirend. The separation of these steps can occur on different levels from very coarse like the separation of the experiment and analysis to very fine where each individual processing step is implemented in a independent script.

Workflow management is a concept to organize the individual steps forming a process. The granularity of the steps to manage highly depend on the complexity of the tasks and the diversity of the processing steps. A common and generic example forming a workflow management system is a queueing system used for cluster computing. Here users submit a number of in the simplest case independent jobs (computing steps) which are then depending on the available resources scheduled and distributed to suitable compute resources. This is a simple example, because the individual processing steps typically do not depend on each other and only the required amount of resources and time needs to be taken into account when organizing the execution.

For scientific projects like the Reach-to-Grasp experiment described in \cref{sec:data} there are dependencies between individual steps of the process from data acquisition to publication (see \cref{fig:scidata_metadata_pipeline,fig:scidata_reachgraspio_diagram}). The workflow management concept has been applied in a number of scientifics fields like genomics or imaging data. In these fields a systematic approach to data processing and analysis is required and feasable, since the they are dealing with large and numerous datasets which exceed manual monitoring or processing power. \todo{add reference to PLI or similar paper}
For these disciplines, there are a number of platforms and tools available to implement pre \& postprocessing as well as analysis processing steps: Galaxy\footnote{Galaxy, \url{https://galaxyproject.org}, RRID:SCR\_006281} an  open, web-based platform providing bioinformatics tools and services for data intensive genomic research, vistrails\footnote{VisTrails, \url{https://www.vistrails.org}, RRID:SCR\_006261} an open-source scientific workflow and provenance management system that provides support for simulations, data exploration and visualization, Taverna\footnote{Taverna, \url{https://taverna.incubator.apache.org}, RRID:SCR\_004437} a scalable, open source \& domain independent tools for designing and executing workflows,  GenePattern\footnote{GenePattern, \url{http://www.broadinstitute.org/cancer/software/genepattern}, RRID:SCR\_003201} a genomic analysis platform that provides access to hundreds of tools for gene expression analysis, proteomics, SNP analysis, flow cytometry, RNA-seq analysis, and common data processing tasks, Renku\footnote{Renku, \url{https://datascience.ch/renku}} an online software platform for reproducible and collaborative data science including workflow management aspects, Terra\footnote{\url{https://terra.bio/}} a scalable platform  for biomedical research for data analysis and collaboration, Ugene \cite{Okonechnikov_2012} a multiplatform open-source software for molecular biology and snakemake\footnote{Snakemake, url{https://snakemake.readthedocs.io/en/stable/}, RRID:SCR\_003475} a Python based language and execution environment for make-like workflows.

In this section we present the usage of snakemake as a workflow management tool as it is domain independent, slim and easily integrates with Python based projects, similar to the Reach-to-Grasp project \cref{sec:data}.

\subsection{Workflow management tools - Snakemake}
Snakemake is a generic worklfow management tool derived from the Make concept\footnote{\url{}} combined with Python features. It is available as conda package\footnote{\url{}} with the latest version $ $, which is applied version within the scope of this manuscript. The description of individual steps of a workflow within snakemake is closely related to Make: A processing step is defined via its input and output files \ref{code:snakemake_simple}. 




\begin{codeenv}
\textbf{Snakemake header}
\inputminted[firstline=1, lastline=3, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/simple_demo.snakefile}
\begin{multicols}{2}
\textbf{Simple rules}
\inputminted[firstline=5, lastline=13, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/simple_demo.snakefile}
\columnbreak
\textbf{Flexible rules}
\inputminted[firstline=15, lastline=23, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/simple_demo.snakefile}
\end{multicols}
\caption[Minimal snakemake example workflow]{Minimal snakemake example workflow. The workflow consists of two rules, for generation of a markdown file (.md) and conversion to a text file by plain copy of the content into a file with .txt extension. There are two versions of each rule demonstrating snakemake features at different complexities: The simple version of the rule handles filenames explicitely, whereas the flexible version of the rule is using wildcards to handle filenames. To resolve ambiguities between the two versions of the rules, we define a rule priority order in the first lines of the snakemake file.}
\label[codelisting]{code:workflows_simple_snakefile}
\end{codeenv}


\begin{codeenv}
\begin{multicols}{2}
\textbf{Snakefile}\\
\inputminted[firstline=1, lastline=40, linenos,tabsize=2,breaklines, fontsize=\scriptsize]{bash}{figures/workflows/python_demo.snakefile}
\columnbreak
\textbf{Environments}\\
\textbf{plotting\_environment.yaml}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{yaml}{figures/workflows/envs/plotting_environment.yaml}
\textbf{data\_generation\_environment.yaml}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{yaml}{figures/workflows/envs/data_generation_environment.yaml}
\textbf{config.yaml}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{yaml}{figures/workflows/config.yaml}
\end{multicols}
\caption[Snakemake example workflow for data generation and plotting]{Snakemake example workflow for data generation and plotting. The workflow consists of three rules, for data generation, data visualization and specification of the all output files of the workflow. The first two rules can be executed in dedicated conda environments, specified via the \code{conda:} directive and shown at the right. The workflow uses a configuration file (Snakefile line 1, \code{config.yaml}), specifying the format for storing \software{Neo} structures. This specification is also used to provide a constraint for wildcards with the name \code{data\_ext}, which resolves ambiguities between the data generation and visualization rule. The rule \code{all}, is by default executed when snakemake is run, it specifies two required output formats of the workflow. For the visualizaion of the workflow diagram when running the \code{all} rule see \ref{fig:python_demo}.}
\label[codelisting]{code:workflows_python_snakefile}
\end{codeenv}

\begin{figure}
    \begin{multicols}{2}
    \todo{Fix include svg with underscores}
%     \includesvg[width=0.4\textwidth]{figures/workflows/python_demo}
    \columnbreak
    \includegraphics[width=0.5\textwidth]{figures/workflows/data}
    \end{multicols}
 \caption[Snakemake example workflow for data generation and plotting]{Snakemake example workflow for data generation and plotting. The workflow diagram (left) and result (right). The workflow consists of two rules of which the \code{plot\_data} rule is executed twice with different parameters to generate the final plot in two file formats (\code{ext: svg}, \code{ex:png}, respecively). Different rules are color coded and the rule name is indicated at the top of each node. The frame style (solid/dashed) indicates if this rule needs to be run to generate a final output file. The arrows indicate the dependencies between the rule executions, rules at the top need to be executed first, since they generate output files that are required as input for the subsequent rules executions.}
\label{fig:python_demo}
\end{figure}


\begin{codeenv}
\begin{multicols}{2}
\textbf{Data generation}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{python}{figures/workflows/generate_data.py}
\columnbreak
\textbf{Data visualization}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{python}{figures/workflows/plot_data.py}
\end{multicols}
\caption[Standalone Python scripts used in \cref{code:workflows_python_snakefile}]{Standalone Python scripts used in \cref{code:workflows_python_snakefile}. The two scripts for data generation and visualization contain generic functions, relying on command line parameters to provide the arguments for the function calls (line 21-24 and line 18-21, respectively). The \textbf{data generation} is split into two functions, one for generation of the \software{Neo} structure (\code{generate\_neo\_data}) and one for saving the \software{Neo} structure to disc (\code{save\_neo\_block}). The first function generates a \software{Neo} \code{Block} containing a single \code{AnalogSignal} with randomly generated data (line 4-12). The second function recieves an generic \software{Neo} \code{Block} and saves it in the format specified by the provided filename (line 16-19). If the script is executed from the command line the input parameter \code{filename} is extracted from the command line arguments and both functions are executed consecutively, passing the \software{Neo} \code{Block} from one function the next (line 21-24). The \textbf{data visualization} uses the same concept as the data generation. Here the two internal functions are loading a \software{Neo} block from the specified data source filename (\code{load\_neo\_block}, line 4-7) and visualize the first \code{AnalogSignal} of a given plot, saving the result in a requested filename (code{plot\_analogsignal}, line 9-16). Both functions are called if the script is called from the command line and the two parameters specifying the data location as well as the output plot filename are extracted from the command line arguments.
}
\label[codelisting]{code:workflows_python_scripts}
\end{codeenv}


The modification time stamps of the files are used for 







\todo{inspiration from computer sciences \& industrial software development: Continuous integration \& deployment}



\subsection{Vision 4 Action}
\subsection{Waves project}
\subsection{Summary \& Guidelines}



\begin{figure}
    \centering
    \todo{Fix include of svgs with underscores}
%     \escapeus{\includesvg[width=\textwidth]{./figures/workflows/rulegraph_colored}}
    \caption[Metadata workflow for Vision4Action experiment]{}
    \label{fig:demo_visualization}
\end{figure}



\todo{workflow management systems: Renku, https://terra.bio/, ugene, from genomics: galaxy, vistrains, Taverna,  GenePattern...}
\todo{introduce the concept of workflow management, snakemake, Apps,  categories
- general apps
- experiment specific apps}
