\clearpage
\chapter[Data representation]{Standardized data representations\\- Making data usable}
\label{sec:neo}
% importance of standardization and global organizations 
For making data interoperable 
For widely accepted and utilized and interoperable data storage and usage, as suggested by the FAIR principles for scientific data management \citep{Wilkinson_2016}, it is necessary to agree on common file formats for storage on disc as well as data representations in memory. The agreement on data standards can develop in a 1) community driven way by groups of users adopting a format and therefore forming the basis for a general spread of the format or 2) industry defined, by companies or organizations defining a standard. In the context of data storage and representation there are numerous standards already defined and maintained. For example the World Wide Web Consortium\footnote{W3C, \url{https://www.w3.org/}} is an international community, that is coordinating the development and maintenance of open standards to ensure the long-term growth of the internet. This includes e.g. the widely spread format scalable vector graphics (svg) format and the Hypertext Markup and Extensible Markup Languages (html, xml, respectively). These standards were developed in a community driven way, and are still continously improved by working groups\footnote{W3C working groups, \url{https://www.w3.org/Consortium/activities}} (e.g. the svg2 format\footnote{svg2, \url{https://www.w3.org/TR/SVG2/}}).


% why this doesn't cover smaller scientific fields
Standards as they are defined by the World Wide Web Consortium affect millions of users and computer systems. Therefore, the issue of standardization forms a foundation for a working system with this large number of participants. On smaller scales however the topic of standardization is not as pressing as individual members are typically able to develop a system, which meets their requirements and often does not require interaction with other systems. At the same time the complexity and specificity of the data and metadata to be represented increases since more detailed information needs to be captured and tracked in a consistent manner. This leads to a an unproportionally high effort required when interacting with other members of the community since the locally established implicite agreements on data storage and handling need to be communicated in addition to the actual data content.

% BIDS as positive example
One way even smaller communities can benefit from standardized data representations from an early stage on is the introduction of standards from the recording system manufacturing side. This situation is more likely to occur if only very few experimental systems exist (e.g. particle physics and the \code{root} format \citep{Brun_1996}) or only few manufacturers produce experimental setups, which simplifies the coordination between these (e.g. medical imaging and the Digital Imaging and COmmunications in Medicine (DICOM) standard\footnote{DICOM, \url{https://www.dicomstandard.org/}}). Based on this, the neuroimaging community managed to extend the standard to coherently organize metadata with data in the Brain Imaging Data Structure\footnote{BIDS, \url{https://bids.neuroimaging.io/}} \citep{Gorgolewski_2016}.

% standards as prerequisite for tool development and vice versa
Standards can emerge from a community or can be established by dominating entities, e.g. in case of a monopoly for hardware production. However, also the development of tools used within and across communities is tightly linked the definition of standards, as these form a prerequisite for interacting with the data. This interaction works both ways: the establishment of standards is important for the development of tools to enable them to get the most information out of the datasets, but also tools can influence the establishment of standards especially in small communities by favouring one format over another and therefore influencing the community preferences. The development of tools applicable across communities requires the prior adoption of global standards, as community internal evolved agreements are typically too diverse to be transferred to other communities to form a basis for a common set of tools. 

% neuroscience: state of the art
A prime example for the delayed recognition for the need of standardization is the development of data formats in the field of electrophysiology. Here specific requirements are needed for when recording electrophysiology datasets: The file formats need to be able to support writing of large amounts of data in a streaming fashion, they have to cope with the custom data structure generated by the recording system  and they need to document the parameters used during the recoding as minimal metadata. This, together with a number of companies developing ready-to-use solutions for experimentalists led to a multitude of file formats (see also \cref{fig:neo_ios}). All systems permit experimentalists to record data without investing years into the development of a recording system but also typically provide only very few supported output file formats. This again restricts the number of easily applicable analysis methods, as the vendor may provide tools for basic visualization, processing and simple analysis the data, but these are typicaly not easily extendable nor do they provide a simple programming interface for the implementation of custom analyses. This unnessarily restricts the scientific questions which can be answered with a particular dataset based on the producer of the hardware system.\\

% neuroshare
A project introducing a common output file format for electrophysiology recording systems is Neuroshare \footnote{Neuroshare, \url{http://neuroshare.org/}}. Version 1.0 was released in 2003 and is a C based library relying on direct-link libraries for the integration with recording systems and providing scripts for the integration in a \software{Matlab} environment. However, neuroshare was only taken up by few vendors of electrophysiology setups, as these are required to provide integration routines for their acquisition system. Additionally, the Neuroshare was mainly developed for Windows based systems, making it complicated to use in combination with other operating systems. In 2010 a Python implementation of Neuroshare was introduced\footnote{python-neuroshare, \url{https://github.com/G-Node/python-neuroshare}}, which is currently not maintained.

Since then, multiple projects attempt to tackle this problem from different angles: The Neurodata Without Borders: Neurophysiology\footnote{\software{NWB:N}, \url{https://www.nwb.org/}} \citep{Teeters_2015} project attempts to define an additional, more generic file format standard aiming to replace a multitude of existing formats. This project was launched in 2014 and a second version was released in 2019 \citep{Rubel_2019}. The project involved multiple scientific laboratories, funding agencies as well as industry partners and provides a fixed set of structures to describe common data encounterd in this collaboration. The \software{NWB:N} format is not supported by major recording setup manufacturers and therefore no file format generated by common electrophysiological recording setups. Instead primary recording data need to be converted into the \software{NWB:N} format.
Another project tackling the same problem is the Python package \software{Neo}. \software{Neo} is a spin-off of the \software{Neurotools} toolbox, an attempt to set up a common file format for neurophysiology setups based on Microsoft dynamic-link libraries. It was initiated in 2009 and in contrast to NWB provides a standardized in memory data representation for electrophysiology data without introducing another file format. Therefore it bridges the gap between available electrophysiology file formats and forms the basis for a number of visualization, preprocessing and analysis tools. By interfacing to a large number of file formats \software{Neo} is the ideal tool for implementing flexible data management workflows independent of the particular file format of the original data files. In addition, it provides also flexibility in the range of software tools that can be used for further data processing as it interfaces with various tools that cover diverse requirements of data processing, visualization and analysis. In the following we will introduce \software{Neo} in more detail and provide example scripts for application of \software{Neo} for handling electrophysiology datasets.



\section{The \software{Neo} Python Package}
\software{Neo}\footnote{\software{Neo}, \url{http://neuralensemble.org/neo}, RRID:SCR\_000634} \citep{Garcia_2014} is an open-source Python package for representing electrophysiology data in working memory. It offers interfaces for reading various electrophysiological proprietary and open file formats and represents the data in a generic way (\cref{fig:neo_ios}). Thus it forms the bases for a number of open software tools: The electrophysiology analysis toolkit\footnote{Elephant, \url{http://neuralensemble.org/elephant}, RRID:SCR\_003833} for analysis of spiking activity and local field potentials, OpenElectrophy\footnote{OpenElectrophy, \url{http://neuralensemble.org/OpenElectrophy}, RRID:SCR\_000819}, SpykeViewer\footnote{SpykeViewer, \url{https://spyke-viewer.readthedocs.io}} and Ephyviewer\footnote{Ephyviewer, \url{https://ephyviewer.readthedocs.io}} for visualization, Tridesclous\footnote{Tridesclous, \url{https://tridesclous.readthedocs.io}} for online and offline spike sorting, NeoAnalysis\footnote{NeoAnalysis, \url{https://github.com/neoanalysis/NeoAnalysis}} \citep{Zhang_2017} for rudimentary visualization and analysis, NetworkUnit \footnote{NetworkUnit, \url{https://github.com/INM-6/NetworkUnit}, RRID:SCR\_016543} for validation testing of spiking networks. Related packages are \software{NiBabel}\footnote{NiBabel, \url{https://nipy.org/nibabel}, RRID:SCR\_002498}, a comparable package for neuroimaging file formats and \software{MNE}\footnote{MNE, \url{https://martinos.org/mne}, RRID:SCR\_005972} an Python package for MEG and EEG analysis and visualization.

The two main features of \software{Neo} are 1) the interfacing to many different file formats, by providing reading capability for numerous proprietary formats and writing capability to selected open formats and 2) the standardized representation of electrophysiology data as a basis for further visualization and analysis steps. Using these features of \software{Neo} is typically used either as conversion tool from specialized to more generic formats or as run time data representation for further processing.

\subsection{Feature updates and current development}
The \software{Neo} version 0.3 was released in 2014 \citep{Garcia_2014}. Since then the software has been extended to be compatible with more data formats, the object model has been revised for better usability and the implementation has been improved for performance. In the following we describe the enhancements introduced between version 0.3 (\cref{fig:neo_architecture03}) and version 0.7 (\cref{fig:neo_architecture07}).

\paragraph{Interfaces to file formats}
\software{Neo} 0.7 is supporting additional file formats for reading, such as Axograph, OpenEphys, Stimfit, Kwik, Nix, Igor, Nest, Neuralynx, NSDF and BCI2000. The capabilities for reading the Axon, Blackrock, Brainvision, Brainware, Elphy, Intan Matlab structures, Neuroshare, Plexon, Spike2, Tdt, NeuroExplorer, Neuralynx, Igor, Elan, Micromed, RawMCS, WinWCP formats have been improved. Reading and writing capabilities have been improved for Nix and Pickle formats. PyNNText and PyNNNumpy formats are no longer supported. A new code design for readers has been implemented and the majority of readers has adjusted accordingly to enable improved loading performance and loading of subsets of data (RawIO implementation). 
\paragraph{Object structure and usability}
The code has been modularized for more flexibility and maintainability, and a large number of unittests have been added. The object structure has been restructured for user friendliness and to boost performance by implementing sets of similar data entities in single objects instead of using individual data objects for each data entity (removal of dedicated array versions of data classes). A new relational container object \code{Channel\_Index} was introduced to simplify the representation of logical relations between data objects replacing \code{RecordingChannel} and \code{RecordingChannelGroup} objects. Consistent deep copy functionality has been added for all data objects and additional internal consistency checks have been added. A new type of custom annotation mechanism has been added, which is designed to capture custom annotations in the same dimension as the data (array annotations). For the installation additional option were introduced, depending on the required file formats which need to be supported. The code style has been adjusted to follow the PEP8 guideline\footnote{Python Enhancement Proposal 8, \url{https://www.python.org/dev/peps/pep-0008}}\citep{PEP8StyleGuideforPythonCode_}. Support for Python 2.6 was dropped and consistent support for Python 3 was introduced.


\paragraph{Outlook}
Practical application of \software{Neo} confirmed an improved usability for version 0.7. The described data objects facilitate data access and performance and the combination of \code{Block} and \code{Segment} objects as container objects provides easy to use access to the data. However, the concept of \code{ChannelIndex} objects is covering too many aspects of relations between data at once: 1) grouping data objects, 2) masking data objects (selection of a subset of data within a data object) and 3) annotating individual samples within data objects. The last aspect has been moved to the individual data objects, by introducing array annotations. 

% groups and views
For future versions, the splitting of \code{ChannelIndex} objects into two separate objects (\code{Group}, \code{View}) responsible for grouping and masking is planned. A \code{Group} object will be able to link different types of data objects, depending on its configuration. For example a \code{Group} object resembling a physical electrode will be able to link to a single \code{AnalogSignal} and multiple \code{Unit} objects. A \code{View} object can be used to refer to a subset of the data stored in a data object (e.g. a single recording trace within an \code{AnalogSignal}). This view can be used instead of a data object in any relation and will provide utility functionality to provide a sliced version of the actual data object.

%linking
Another topic of discussion is the linking between \software{Neo} objects. Up to the current version 0.7 all links between \software{Neo} objects can be established bidirectional. However, the bidirectionality of the linking is not inherently guaranteed, since the generation of link does not automatically generate a backward link between \software{Neo} objects. Introducing automatic bidirectional linking would guarantee bidirectional linking, but might complicate the set up of a \software{Neo} structure e.g. when reading a data file. There are different approaches possible to circumvent these problems: i) The use of only unidirectional links from higher level to lower level objects (top to bottom). This approach would still provide most of the functionality commonly used. ii) The implementation of a validation framework, which can on request check if a provided \software{Neo} object structure is fully linked including consistent bidirectional links. A suggested model implementing the first of the two suggestions is presented in \cref{fig:neo_architecture_future}. Here links from \code{View}s and \code{Group}s towards data objects are unidirectional, therefore preventing cyclic links across the complete \software{Neo} structure.

%Imaging data
Spiking activity of individual neurons can not only be recorded using sharp electrodes, but also  from multielectrodearray recordings (MEA) and calcium image recordings \citep{Kelly_2007, Shew_2010}. To support the usage of \software{Neo} also for imaging data an extension of \software{Neo} by two additional object types is planned: the \code{ImageSequence} object will capture sequences of regularly sampled images and is therefore closely related to the \code{AnalogSignal} as it contains the same type of data, but also captures the spatial relation between different pixels (traces). A second object relevant for image handling is a \code{RegionOfInterest} object, which is used to spatially mask a specific part of a stack of images. The \code{RegionOfInterest} can implemented as special case of a \code{View}. Support for \code{ImageSequence}s also requires capabilities to read imaging data formats, which will be added subsequently.


\subsection{\software{Neo} Object Structure}
% data vs container objects, general attributes: name, description, file\_origin
\software{Neo} objects can be separated into two types: data objects, describing basic recording data in combination with minimal metadata and container objects, providing the structural framework for the relation between the data objects. In general, all \software{Neo} objects have three optional arguments to provide custom information about the captured data: 1) \code{name} attribute can be used to label the object and can be used for simple data filtering and selection. 2) The \code{description} attribute is intended to provide a human readable, detailed, 1-2 sentence description for the data contained / grouped by the \software{Neo} object. The \code{file\_origin} is can be used to describe the origin of the data, e.g. the original recording filename or simulation script. In addition to \code{name}, \code{description} and \code{file\_origin}, all \software{Neo} objects can capture additional custom information in form of a \code{annotation} dictionary. This dictionary can contain arbitrary data in all basic Python data types as well as \code{datetime}, \code{date}, \code{time}, \code{timedelta} in arbitrary structures build from lists, dictionaries, tuples or \software{Numpy} arrays without any restrictions on the shape of these objects.


\paragraph{Data Objects}
Data objects are based on \software{Numpy} arrays \citep{Walt_2011} for efficient computation on large datasets. In addition, \software{Neo} objects are aware of physical quantities by using the \software{Quantities} package \citep{Dale_}.
% Continuously Sampled Signals
\software{Neo} provides data objects to capture regularly as well as irregularly sampled continuous signals in \code{AnalogSignal} and \code{IrregularlySampledSignal} objects, respectively. Both objects rely on a 2 dimensional \software{Quantities} (\software{Numpy}) array capturing the basic data signal, whereas the first dimension describes the time and the second dimension different signal traces. For \code{IrregularlySampledSignal}s time information is captured in a second separate \software{Quantities} array, sharing the first dimension with the signal array. For \code{AnalogSignal}s this in implemented in a more compact fashion by storing only the sampling rate (\code{sampling\_rate}) and the starting time point of the recording (\code{t\_start}). A \software{Quantities} array containing the time values corresponding to the data point can be generated on request via the \code{times} attribute of the \code{AnalogSignal}.
% Time series data
For time series data \software{Neo} provides a \code{SpikeTrain} object, capturing the data in a \code{times} attribute. Additionally the start and stop times of the data acquisition are essential for the interpretation of the data, these are provided as mandatory \code{t\_start} and \code{t\_stop} scalar \software{Quantities} parameters. Optionally, a \code{SpikeTrain} object can also capture snippets of regularly sampled continuous signal around each time point in the waveform attribute. This links to a 3 dimensional array, capturing the time, spike ID and recording channel dimensions of the waveforms. The \code{SpikeTrain} attribute \code{sampling\_rate} is used to capture the corresponding sampling information as in the \code{AnalogSignal} case. To relate the waveform snippets with the time series data the \software{Quantities} scalar \code{left\_sweep} defines the constant offset between the time series data and the corresponding waveform snippet.
% Events and Epochs
A different type of time series data is non-neuronal time series which describe specific time points or durations during the recording of neuronal activity. These might be control signals of the experiment, e.g. trial start times, behavioral events of the subject, e.g. a the time in which a button was pressed. For the description of time series data \software{Neo} provides \code{Event} objects, capturing the data in a one dimensional \software{Quantities} array (\code{times} attribute) together with a string array of the same shape (\code{labels} attribute) providing labels to the individual time points. To represent extended periods of time, \software{Neo} offers \code{Epoch} objects having the same attributes as \code{Event}s, and in addition a one dimensional \software{Quantities} array of durations with the same shape as the main time series data.

% Array annotations
All of the above mentioned data objects consist of a main \software{Quantities}-wrapped \software{Numpy} array with one or more dimensions. One of the latest features introduced in \software{Neo} is to implement a second kind of annotation mechanisms on this. The annotations of an object always refer to the object as a whole. However in many use cases annotations have been used to provide details about the individual data samples by containing arrays, which share some dimensions with the main data samples. For example \code{AnalogSignals} which contain more than one recording trace are frequenly annotated with list providing detailed information about the individual recording channels, e.g. the identity of the channel or the particular filter settings. However, when modifying the shape of the original data, these annotations can not be updated in an automatic fashion since they were user defined and didn't follow a fixed schema. Since \software{Neo} version 0.7.0 all data objects have an additional feature \code{array\_annotation}s, which fills this gap by providing an annotation mechanism for capturing sample based annotations, i.e. annotation entries with the same length as the main data dimension. These \code{array\_annotations} are automatically adjusted when the shape of the main data is modified, e.g. by slicing the data in the time axis or extracting a single signal trace from an \code{AnalogSignal}.

% Lazy loading
The \code{Neo} 0.7 release also features a standardized way of optionally loading specific parts of the data on request. This is of advantage when dealing with datasets which are large in comparison to the available memory. The new \code{lazy} feature permits to generate the complete \software{Neo} structure (\cref{fig:neo_architecture07}), but substituting all data objects with proxy objects, which feature the same attributes and links as classic data objects, but do not contain the actual data. For accessing the data a \code{load} mechanism is provided which loads the requested parts of the data and provides them in a separate classic data object not linked to the main \software{Neo} structure. Using the \code{lazy} mechanism large datasets can be processed chunkwise without requiring large amounts of memory.

\paragraph{Container Objects}
% Block and Segment
\software{Neo} container objects provide the structural relations between \software{Neo} data objects. The base object for a dataset is a \code{Block} object, containing everything related to the dataset. The \code{Block} object can link to a number of \code{Segments} and \code{ChannelIndex} objects which can be used to organize data objects according to their timing, spatial relation or custom grouping aspects, e.g. grouping the data objects by the signal quality of the contained data. \code{Segment} objects are intended for grouping objects which share the same time frame, e.g. simultaneously recorded \code{AnalogSignal}s and \code{SpikeTrain}s (\cref{fig:neo_architecture07}).
\code{SpikeTrain}s from different \code{Segment}s that are considered coming from the same source (neuron) can be linked across \code{Segments} via \code{Unit} objects.
% Units and ChannelIndexes
A \code{Unit} object again can be grouped together with \code{AnalogSignal}s and \code{IrregularlySampledSignal}s in a \code{ChannelIndex} object. In addition to the grouping functionality, a \code{ChannelIndex} object also provides an additional labeling functionality for child \code{AnalogSignal}s via the \code{channel\_ids} and \code{channel\_names} attributes. These consist of one dimensional integer and string arrays labeling the individual traces of the attached continuous signals. Via the mandatory attribute \code{index} a selection of the traces within the linked continuous signals can be done. However, this requires a consistent ordering of recording traces within all linked continuous signals of a \code{ChannelIndex}.
All container objects provide utility functions to facilitate access and selection of data objects. This is implemented in the form of a \code{filter} method, which returns a list of \software{Neo} objects based on a combination of object type, attribute and annotation constraints.


\begin{figure}
    \centering
    \includestandalone[mode=image|tex, width=\textwidth, obeyclassoptions=true]{./figures/neo_ios_and_tools}
    \caption[Neo embedding]{\software{Neo} embedding. \software{Neo} 0.7. supports a number of file formats for reading (light blue) and writing (dark blue). Many of the supported formats can be read in a improved fashion, permitting for more efficient memory usage (black frames).  Neo provides an interface for many advanced tools for visualization, simulation, analysis and data storage.}
    \label{fig:neo_ios}
\end{figure}


\begin{figure}
    \centering
    \includesvg[width=\textwidth,pretex=\relscale{0.8}]{./figures/neo07_schema.svg}
    \caption[\software{Neo} 0.7 object structure]{\software{Neo} 0.7 object structure. Figure modified from \url{https://github.com/neuralensemble/python-neo}.}
    \label{fig:neo_uml}
\end{figure}

\begin{figure}
    \centering
    \software{Neo} 0.3 architecture
    \includesvg[width=\textwidth]{./figures/neo_architecture03.svg}
    \caption[\software{Neo} 0.3 architecture]{\software{Neo} 0.3 architecture. \software{Neo} represents electrophysiology signals in data objects such as \code{AnalogSignal}s, \code{AnalogSignalArray}s, \code{IrregularlySampledSignal}s, \code{SpikeTrain}s and \code{Spike}s, whereas the latter two optionally include information about waveform data for each spike. Additional supplementary information describing the timing during the recording can be provided using \code{Event}s and \code{EventArray}s or \code{Epoch}s and \code{EpochArray}s to mark time points or durations during the recording, respectively. All above described data objects are put into relation by container objects, such as \code{Segment}s (grouping all data objects simultaneous in time), \code{Unit}s (grouping \code{SpikeTrain}s and \code{Spike}s across time), and \code{RecordingChannel}s and \code{RecordingChannelGroup}s (grouping \code{AnalogSignal}s \code{IrregularlySampledSignal}s and \code{Unit}s, \code{AnalogSignalArray}s and \code{RecordingChannel}s, respectively). The top level container is a \code{Block} linking to \code{Segment}s and \code{RecordingChannelGroup}s. Figure from \url{https://neo.readthedocs.io/en/0.3.3}.}
    \label{fig:neo_architecture03}
\end{figure}


\begin{figure}
    \centering
    \includesvg[width=\textwidth]{./figures/neo_architecture07.svg}
    \caption[\software{Neo} 0.7 architecture]{\software{Neo} 0.7 architecture. \software{Neo} represents electrophysiology signals in data objects such as \code{AnalogSignal}s, \code{IrregularlySampledSignal}s and \code{SpikeTrain}s optionally including information about waveform data for each spike. Additional supplementary information describing the timing during the recording can be provided using \code{Event}s or \code{Epoch}s to mark time points or durations during the recording, respectively. All above described data objects are put into relation by container objects, such as \code{Segment}s (grouping all data objects simultaneous in time), \code{Unit}s (grouping \code{SpikeTrain}s across time) and \code{ChannelIndex}es (grouping \code{AnalogSignal}s \code{IrregularlySampledSignal}s and \code{Unit}s). The top level container is a \code{Block} linking to \code{Segment}s and \code{ChannelIndex}es. Figure modified from \url{https://github.com/neuralensemble/python-neo}.}
    \label{fig:neo_architecture07}
\end{figure}


\begin{figure}
    \centering
    \includesvg[width=\textwidth]{./figures/neo_architecture_future.svg}
    \caption[Proposed \software{Neo} architecture]{Proposed \software{Neo} architecture. The proposed \software{Neo} architecture preserves all objects from \software{Neo} version 0.7 except for \code{ChannelIndex}es and \code{Unit}s. These are replaced by \code{Group} and \code{View} objects, which a more generic, but still customizable way of organizing data objects. \code{View} objects can mask data objects by linking to a subset of the contained data (e.g. a single trace of an \code{AnalogSignal}. This linking is unidirectional, preventing complex dependencies involving data objects. \code{Group}s are capable of linking to any kind of data objects or \code{View} of data objects. The required specificity is provided by the different \code{mode}s of a \code{Group} object. These limit to the connected objects to a specific type and number, wherefore a \code{Group} can e.g. be used instead of a \code{Unit} object. \code{Group}s can also link to other \code{Group} objects to provide higher level organization of the data.}
    \label{fig:neo_architecture_future}
\end{figure}


\section{\software{Neo} Usage Examples}
In the following we demonstrate in three practical examples how \software{Neo} can be used to load data from different file formats in a memory efficient manner, access and select data, annotate and filter data according to custom metadata added and save data in an open source format.

\subsection{Loading \& Visualization}
Loading data in \software{Neo} is implemented in two stages: First initialization of the reader (IO) and second reading of the \software{Neo} structure. For readers implemented in the standardized manner (\cref{fig:neo_ios}, black frame) you need to provide the filename of the dataset to load. This will generate an \code{io} object providing functionality to load the data into a \software{Neo} structure. Depending on the file format either a \software{Neo} \code{Block} or a \software{Neo} \code{Segment} can be loaded using the \code{read\_block} or \code{read\_segment} method of the \code{io} object.

In this example, the published dataset described in \cref{sec:data} is used for demonstrating the loading of electrophysiology data into the \software{Neo} structure (\cref{code:neo_visualization}). Data were previously downloaded from GIN\footnote{\url{https://gin.g-node.org/INT/multielectrode_grasp}} and continuous as well as sorted spiking data are loaded using the \software{Neo} \code{BlackrockIO} class. This IO provides standardized access to the data, permitting lazy loading of \software{Neo} objects [cf. \cref{code:neo_visualization} line 10f]. Here the the \software{Neo} filter functionality is used to select all \code{SpikeTrain}s which have an annotation key \code{'channel\_id'} and the corresponding value of the user requested channel index (\code{selected\_channel}, line 13). In the next step, the corresponding \code{AnalogSignal} trace is extracted by finding the \code{AnalogSignal} with a corresponding in the \code{array\_annotation} with key \code{channel\_ids} and extracting the \code{id} of the corresponding trace (line 15-18). Finally, the analog and spiking data of 10 seconds of recording are loaded into memory via the \code{load} mechanism of the respective data objects and returned by the \code{load\_single\_channel\_data} function (line 19-22).

The visualization of a single \code{AnalogSignal} trace together with spiking activity from multiple \code{SpikeTrain}s is implemented in the \code{plot\_data} function. This requires the corresponding data objects as input as well as a location to store the final scalable vector graphics plot (line 25). Here, \software{Matplotlib} \citep{Hunter_2007} is exploited to visualize the electrophysiological data (line 30 \& 34-38). Correct scaling of the signals and automatic generation of axis labels is ensured by the inherent use of the \software{Quantities} package within \software{Neo} (line 28-31, 34-37). The resulting plot is exported to the scalable vector graphics (svg) format for storage in a flexible, memory efficient and scalable manner. Finally, user specific setting are extracted from command line parameters and both functions are executed sequentially to generate a visualization of a single recording trace and corresponding spiking activity.




\begin{codeenv}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{python}{figures/code_visualize_data.py}
\caption[Data loading and visualization with \software{Neo}]{Loading data into the \code{Neo} framework and visualization using the Matplotlib package. Required packages are imported (line 1-4) and two main functions are defined for loading and visualization of the \software{Neo} structure: \code{load\_single\_channel\_data} and \code{plot\_data}. The first one requires the location of the datasets and the selected channel to plot as command line arguments (line 9). Here, the dataset is opened using the \code{BlackrockIO} in \code{lazy} mode and the user-specified channel is selected and loaded into memory together with the spiking activity (lines 7-22). The second function visualized a given \code{AnalogSignal} together with a list of \code{SpikeTrain}s using the \software{Matplotlib} package (line 25-40). Finally both functions are run subsequently using command line parameters for dataset and channel specifications. For the resulting plot see
\cref{fig:demo_visualization}}.
\label[codelisting]{code:neo_visualization}
\end{codeenv}

\begin{figure}
    \centering
    \includesvg[width=\textwidth]{./figures/i140703-001.svg}
    \caption[Data visualization example]{Visualizating of the activity recorded on a single electrode. A single trace of an AnalogSignals contains the voltage samples of a recording electrode (blue). The corresponding threshold crossing events are split into three different \code{SpikeTrain} objects and time stamps are marked as vertical lines with a \code{Unit} specific offset (orange, green, red).}
    \label{fig:demo_visualization}
\end{figure}

\subsection{Annotation of data with metadata from \software{odML}}
Annotations are a key feature of the generic \software{Neo} structure to provide the necessary customizations to be used for a specific dataset. Standard annotations are a feature of all \software{Neo} objects and can be accessed via the \code{annotation} attribute. In addition, \software{Neo} data objects also provide a more specific type of annotation mechanism, namely \code{array\_annotations} which act in the same manner as regular \code{annotations} but are directly coupled to the dimension of the underlying data. This permits to store metadata which are linked to individual data entries and handle them in an automatic way when manipulating the data object. This example demonstrates how to access and modify regular and array annotations using a metadata collection in the odML format.

\cref{code:neo_annotation} demonstrates two aspects of \software{Neo} annotations: firstly annotation access via the object attributes \code{annotation} and \code{array\_annotation} and secondly the generation of (array) annotations. The resulting script output is shown in \cref{code:neo_annotation_print}. The dataset is loaded in the same manner as in \cref{code:neo_visualization} and \code{annotations} and \code{array\_annotations} of selected objects are printed using the \code{print\_annotations\_example} functions. The resulting printout shows the different levels of annotations, which are automatically generated by \software{Neo} when loading a dataset from the \code{Blackrock} format. Annotations on the \code{Block} level provide general information about the recording session, whereas \code{SpikeTrain}s carry very specific metadata about the identity of the object and its spike sorting classification.

An example for \code{array\_annotations} is provided for an \code{AnalogSignal}. \code{AnalogSignal} objects provide the most metadata as they describe all recording traces they contain individually (see \cref{code:neo_annotation_print} line 15-33). Here the \code{AnalogSignal} contains 96 recording traces and all array annotations have a matching length of 96. The annotated information covers the identity of the electrodes ('channel\_ids' and 'channel\_names', line 16f) and identity within the recording system ('connector\_ID' and 'connector\_pinID', line 19f) as well as signal processing parameters for signal filtering and spike extraction (line 21-30). Here the keys consist of a combination of the file type affected (nsx/nev), the filter border (high/low), the general parameters (freq/energy\_threshold/dig(itization)\_factor/waveform\_size) and filter parameters affected (corner/order/type). In addition, a human readable description as well as information about the originatign file is provided ('description', 'nsx' and 'file\_origin', line 18, 31f). All \code{array\_annotations} will automatically be adjusted when the underlying data object is modified via \software{Neo} functions, e.g. via slicing in time.

The second part of the example script (\cref{code:neo_annotation}, line 28ff) demonstrates the generation of additional (array) annotations by loading a metadata collection in the odML format, extracting relevant information for the interpretation of an \code{AnalogSignal} and adding this information to the \code{AnalogSignal} as array annotation. First, an odML file is loaded using the Python \software{odML} library (line 31) and all \code{Sections} describing electrodes are extracted to load a mapping between the Blacrock channel \code{ID}s provided by the recording system and the spatially ordered \code{ConnectorAlignedID}s (line 32f), which are defined in order to indicate an electrode's spatial position easily. In the next step, the existing Blackrock \code{ID} array annotations of an \code{AnalogSignal} are used to generate the corresponding array of \code{ConnectorAlignedID}s using the previously extracted mapping b(line 36f). Finally, the new ids are added as a new array annotation to the \code{AnalogSignal} using the \code{array\_annotate} mechanism (line 38).

The generated array annotations are displayed by printing the complete array annotations of the \code{AnalogSignal} again, resulting in \cref{code:neo_annotation_print} line 43ff. Here, the new key 'connector\_aligned\_ids appears containing the corresponding spatially organized ids of the recording electrodes.

With array annotations and annotations, \software{Neo} now offers a mechanisms to provide custom information for \software{Neo} objects as a whole, but also for subsets of data contained by \software{Neo} objects. With these annotation mechanisms it is now possible to directly add \software{odML} content to \software{Neo} objects on multiple levels of the data organization. However, this annotations are not automatically generated yet, since there is no mechanism assigning data in the \software{Neo} structure and metadata in the \software{odML} structure. Due to the generality of \software{Neo} and \software{odML} structure the is no generic assignment strategy between the two structures possible, but these relations need to be captured explicitely using an extended framework.


\begin{codeenv}
\inputminted[linenos,tabsize=2,breaklines, fontsize=\scriptsize]{python}{figures/code_annotate_data.py}
\caption[Annotation access and editing with \software{odML} and \software{Neo}]{Annotation access and editing with \software{odML} and \software{Neo}. Required packages are imported (line 1) and three functions for printing \code{annotations} and \code{array\_annotations} are defined (line 3-25). \code{pretty\_print\_dict} provides functionality for displaying individual dictionary items in separate lines (line 3-6). \code{print\_annotations} uses the previous function to print based on the \code{mode} keyword the \code{annotations} or \code{array\_annotations} of a given \software{Neo} object. \code{print\_annotation\_example} prints a selection of \code{annotations} and \code{arra\_annotations} from a \software{Neo} block structure. The function \code{generate\_annotations\_from\_odml} extracts metadata information about the mapping of different types of \code{id}s from an \code{odML} file and annotates the corresponding \code{AnalogSignal} (line 27-37). Finally, all functions are demonstrated based on a command line specified data- and metadataset (\code{data\_location} and \code{odml\_filename}, line 40-45). The generated annotations are confirmed via a final call of \code{print\_annotations} (line 46f).}
\label[codelisting]{code:neo_annotation}
\end{codeenv}

\begin{codeenv}
\inputminted[linenos,tabsize=2, fontsize=\scriptsize, style=friendly]{cucumber}{figures/i140703-001_annotations.txt}
\caption[Output of \cref{code:neo_annotation}]{Output of  \cref{code:neo_annotation}. Listed are the automatically generated annotations of example \software{Neo} objects as extracted from \code{Blackrock} recording files. On the \code{Block} level these are dealing with general information about available files and interruptions in the recording process. For \code{SpikeTrain}s there is information provided about the identity and classification of the \code{Unit} the \code{SpikeTrain} was assigned to. For \code{AnalogSignal}s there are a arrays describing the attributes of the individual electrode traces. This includes information about the electrode identities (\code{channel\_ids}), mappings of contacts within the recording system (\code{connector\_ID}s, \code{connector\_binID}s) and the applied filter and threshold settings for signal preprocessing and spike extraction.}
\label[codelisting]{code:neo_annotation_print}
\end{codeenv}


\subsection{Saving Data \& Format Conversion}
\label{sec:nix_format}
Being able to store intermediate preprocessing steps or analysis results in a persistent manner is important for making workflows reproducible. \software{Neo} provides the option to store data in plain Ascii, KlustaKwik \citep{Hazan_2006}, Nix \citep{Stoewer_2014}, binary Matlab, Neuroscience Simulation Data Format (NSDF) \citep{Ray_2016}, binary Python pickle and a custom binary format, see also \cref{fig:neo_ios}. In this example we focus on the Nix format as it provides the most versatile data storage. Nix is based on an hdf5 \citep{TheHDFGroup_1997} backend which provides the flexibility of fast and memory efficient access for large datasets. In addition, Nix is designed to capture data as well as metadata, providing the opportunity to unify both in a single file and add links between relevant metadata and the corresponding data. 
This example demonstrates the conversion of the dataset used already in previous examples from the \code{Blackrock} format to the \software{Nix} format. Furthermore, it showcases the addition of metadata from the \code{odML} format to the same file and the extraction of the data from the \software{Nix} file.
\cref{code:neo_saving} provides four functions for the back and forth conversion of files in the \software{Neo} and \software{odML} format to the \software{Nix} format (line 4-24). These functions are mostly based on the open-source \software{nix-odML-converter} library \footnote{\url{https://github.com/G-Node/nix-odML-converter}}. This library can be used for simple command line conversion between \software{Nix} and \software{odML}. Here we demonstrate the usage of the Python interface, permitting a more flexible approach to conversion parameters (e.g. filenames, write modes).
We demonstrate the merging of the data together with the metadata into a single \software{Nix} file using the first two functions (\cref{code:neo_saving} \code{save\_neo\_to\_nix} and \code{save\_odml\_to\_nix}). The last two functions deal with the extraction of the corresponding information of the individual components from the \code{Nix} file (\cref{code:neo_saving} \code{load\_odml\_from\_nix} and \code{load\_neo\_block\_from\_nix}).

With the capability of the \software{Nix} format to capture data as well as metadata in a combined fashion both modalities can be comprehensively stored in a common framework. Integrating both modalities in the \software{Nix} format is easy since \software{Neo} is supports the writing of data in the \software{Nix} format and metadata can be supplemented using the \software{nix-odML-converter}.

\begin{codeenv}
\inputminted[linenos,tabsize=2, fontsize=\scriptsize]{bash}{figures/code_saving_data.py}
\caption[Saving data and metadata to \software{NIX}]{Saving data and metadata to \software{NIX}. Required packages are imported (line 1f) and four functions for the conversion between \software{Neo} and \software{odML} on the one side and \software{Nix} on the other side are defined (line 4-24). The main part of the script (line 26ff) first extract command line specified parameters (location of the dataset and \software{odML} and \software{Nix} filenames to be used) and loads the datasets from the \code{Blackrock} format. Next, the \software{Neo} block in converted to the \software{Nix} format (line32) and the complete metadata collection is added from an \software{odML} file (line33). Finally, we demonstrate how to extract the corresponding information again from the \software{Nix} format using the \code{load\_odml\_from\_nix} and \code{load\_neo\_block\_from\_nix} functions (line36f). Please note that the execution time of this code highly depends on the size of the dataset, since for sequentially all parts the complete dataset will is loaded into memory.}
\label[codelisting]{code:neo_saving}
\end{codeenv}


\section{Comparison of \software{Neo} to \software{NWB:N}}
The \software{NWB:N} format \citep{Teeters_2015} is an alternative approach to \software{Neo} for handling of electrophysiology data. In contrast to \software{Neo} it specifies a new file format instead of providing interfaces to existing formats. More precisely, \software{NWB:N} provides a modular framework for the capture of specific electrophysiology modalities. %https://pynwb.readthedocs.io/en/stable/overview_software_architecture.html
It is accompanied by a number of canonical schema (type) for capturing specific types of data, e.g. behavioural and time series data, imaging recordings, extra and intracellular electrophysiology recordings as well as optogenetic stimulation and optical physiology. Custom schema can be specified to capture data modalities not covered by the set of canonical schema.  % https://nwb-schema.readthedocs.io/en/latest/
This makes the \software{NWB:N} format on the one side more generic than the \software{Neo} structure, since it permits the implementation of custom schema specifications, but on the other side also more restrictive, because existing schema specifications are very specific and can not be reused in a slightly different context. Also implementation of a custom schema specification extensions in \software{NWB:N} requires additional background knowledge of the experimenter about the underlying organization of the \software{NWB} package and the time and effort of implementing such a format. %https://nwb-schema.readthedocs.io/en/latest/format_description.html#extending-the-format
For \software{Neo} the approach is different: the underlying object structure is as generic to automatically cover most of the electrophysiology data and the experiment specificity is introduced via the annotation and array annotation mechanism. This implies that the user only has to understand the \software{Neo} objects once and can reuse the knowledge also in different experimental contexts.
\software{Neo} can deal with most of the available electrophysiology file formats and will be extended to cover calcium imaging data in the future. For \software{NWB:N} there are very limited conversion tools available to convert from specific \software{Matlab} structures and custom formats to the \software{NWB:N} format. %https://github.com/NeurodataWithoutBorders/exp2nwb  https://github.com/NeurodataWithoutBorders/mat2nwb https://github.com/NeurodataWithoutBorders/nwbn-conversion-tools
An interface for the support of additional formats exists, but format converters are in an early development stage. At this point \software{NWB:N} could benefit from integrating \software{Neo} to gain basic support for many electrophysiology formats.
\software{NWB:N} has reference implementations in Python as well as Matlab, suiting most of the neuroscientists, whereas \software{Neo} is only implemented in Python, focussing on non-commercial availability of electrophysiology analysis software.


\section{Summary}
We introduced the \software{Neo} Python package designed for standardized and efficient representation of electrophysiological data. We presented the development of the package since the original publication and depicted the main features and flexibility of the package in three small usage examples. Finally we provide a concise comparison to the recent \software{NWB:N} format and higlight the strengths of the software packages.
Using the \software{Neo} package provides a sound foundation for organizing research data according to the FAIR priniciples as \software{Neo} i) makes data accessible by supporting the conversion from various file formats to the standardized \software{Neo} data representation, ii) is an open, freely available und software package and iii) it makes data interoperable by providing a formal, accessible and broadly applicable basis for data representation. In addition \software{Neo} is supporting the \software{Nix} file format, which is fulfilling additional FAIR principles. These are i) making the data within the file format findable by using globally unique identifiers for data and metadata objects, ii) permiting the annotation with rich metadata as extensive metadata collections can be stored, iii) connecting data and metadata in a formal way by storing links between data and metadata objects, iv) being an open and free \& interoperable framework.
Additional aspects of the FAIR principles, like provenance tracking of the data and metadata, require the embedding of \software{Neo} and \software{Nix} in a data and metadata pipeline or workflow.


