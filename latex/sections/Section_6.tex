\clearpage
\chapter{Discussion}
\label{sec:discussion}

The field of experimental neuroscience deals with fundamental questions concerning the functioning of the brain. To answer these questions, experimental data from many different modalities are collected in complex experiments and subsequently combined in analyses. A cornerstone of the scientific method and a prerequisite for derived scientific knowledge, however, is a complete and comprehensive description of the collected data. We present the current state of data and metadata management in the neurosciences based on a complex experiment and identify key issues. We address these by presenting software solutions and tools for simplified and consistent data and metadata handling. This includes the simplified and standardized aggregation of metadata as well as the consistent presentation of data and the systematic structuring of workflows. We extend this by presenting a second, more complex experiment and demonstrating the integration of the tools to form a complete, reproducible workflow.

In \cref{sec:data} to \cref{sec:workflows} we present information processing and related tools and principles in the context of making complex electrophysiological experiments accessible, understandable and usable in a collaborative setting following the concepts described in \citet{Zehl_2016, Zehl_2018}. We introduce the FAIR principles for scientific data management and stewardship and evaluate the presented tools and approaches based on these. We describe two experiments and their corresponding data and metadata processing pipelines. In doing so, we focus on the evolution of the approach used in the second experiment based on a rigorous evaluation of the first.

The two presented experiments thus demonstrate the progression of approaches for data management in this context. The first example, described in \cref{sec:data}, is an electrophysiological experiment involving a macaque monkey trained for a reach to grasp task (Reach-to-Grasp experiment). The recording encompasses high-resolution neuronal data as well as online and offline processed data together with task control and behavioural signals. The metadata in this project was collected retrospectively and had to be extracted from a variety of different file formats involving different preprocessing steps in part manually by scientists. We describe the pipeline which was used to create a comprehensive metadata collection for two published datasets and discussed limitations of the chosen approach. Based on the described metadata pipeline we identify essential requirements for the data and metadata handling in complex, collaborative projects.

In \cref{sec:metadata} we present \software{odMLtables}, an open-source tool we developed to aid scientiststo collect metadata in a standardized format in their daily routines. \software{odMLtables} facilitates the interaction with the hierarchical, \code{xml} based \software{odML} format by converting between \software{odML} and a tabular representation of the metadata, making the \software{odML} format accessible with widely used spreadsheet software. In addition, \software{odMLtables} implements a number of functionalities that we identified as common steps in creating and using metadata collections. This is an important step for metadata collection, since typically a large fraction of metadata is collected neglectfully and in non-standardized formats.

A comprehensive metadata collection is essential to interpret the data. However, also data require a standardized representation for making them easily accessible to scientists. In \cref{sec:neo} we introduce the \software{Neo} software, which provides a standardized, generic data representation for electrophysiological data. We discuss the evolution of the \software{Neo} data representation and highlight the most important features, such as the support for numerous proprietary and open electrophysiological file formats and the generic structure with a flexible annotation mechanism for custom description of the data. For this purpose, we demonstrate the usage of \software{Neo} in three scenarios and list open source tools building on the \software{Neo} structure.

Finally, we introduce the second electrophysiological experiment, conducted after the completion of the Reach-to-Grasp experiment, which is named Vision-for-Action and features a more complex experimental design. This increase in complexity required us to reevaluate our data and metadata workflows in light of the limitation identified in \cref{sec:data}. Here, neuronal signals are recorded from two cortical areas at once using two parallel, synchronized recording setups. In addition, the monkey is trained to track visual targets in a horizontal plane with a manually operated cursor while his eye and hand movements are recorded simultaneously. The organization of data and metadata in this experiment is designed for facilitated access and organization in later processing steps. We implement a comprehensive metadata workflow integrating metadata in a modular, automatized fashion using the \code{snakemake} workflow management system and describe the combination of data and metadata into user-friendly data packages. We evaluate the new workflow based on the requirements identified in \cref{sec:data} and provide generalized guidelines for the design of future projects.

Combining all of the presented approaches and tools, we suggest a general schema of scientific data and metadata handling from generation to publication (\cref{fig:disc_general_schema}) supporting the implementation of the FAIR principles. In this approach data and metadata are packaged into a common format using a modular workflow, as implemented in the Vision-for-Action project. This workflow can be automatically initialized upon any change in the underlying data or metadata (such as the addition of new data files or changes in the preprocessing steps) and generates a packaged version of the data which is then deployed to a central repository via a continuous integration system. This leads to fast, efficient continuous deployment of data, that is guaranteed to be up-to-date. From there scientists can access the packaged versions and run different analysis or publish the datasets to make them available to the global scientific community, without the need to supply additional custom codes.

The tools presented in this manuscript can be applied at all stages of this schema:
\begin{description}
 \setlength\itemsep{0pt}
 \item[\software{odML}] provides the basic structure for metadata storage during compilation. The final workflow outputs, the packaged data and metadata in the \software{Nix} format, are consistent with this representation. \software{odML} supports the implementation of the FAIR principles by permitting the definition of common terminologies and providing a standardized, accessible, searchable structure for metadata storage. 
 \item[\software{odMLtables}] is used for converting metadata into the standardized \software{odML} representation and can be used for visual inspection and exploration of the metadata collection in a tabular format at any stage. It supports the implementation of the FAIR principles by facilitating the usage of \software{odML} in laboratory environments, thereby supporting the spread of standardization of metadata.
 \item[\software{Neo}] is used for data conversion and standardized representation during preprocessing, compilation and analysis steps. It supports the implementation of the FAIR principles by providing a standardized data representation and conversion to the \software{Nix} format.
 \item[\software{Nix}] is used for combined storage of data and metadata in a standardized, interoperable format adhering to the FAIR principles  by implementing persistent identifiers for indexing of data and metadata.
 \item[\software{Elephant}] functionality can be used during preprocessing steps as well as during later analysis steps. \software{Elephant} contributes to prepocessing steps according to the FAIR principles and builds on data adhering to the FAIR principles and can be used to implement reproducible data analysis using these data.
 \item[\software{snakemake}] is a workflow system which is used for implementing the preprocessing, compilation, integration, packaging and deployment workflow and can as well be used for systematic data analysis at later stages. Utilizing \software{snakemake} forms a robust foundation for data and metadata management based on the FAIR principles.
 \item[\software{Gin}] is a git-annex based repository, that can be used for hosting the original raw data and metadata to provide version control and a basis for a continuous integration system. At the same time, it is our service of choice for version controlling and hosting the packaged metadata and can serve as a platform for publishing the data as it provides a digital object identifier (DOI) service. Also, it allows local deployment on the server at an institution. \software{GIN} provides a service to make data findable on a global scale according to the FAIR principles by providing storage and digital object identifiers for data publication.
\end{description}
 All presented tools fulfill the FAIR principle of being open and free. The majority of the presented tools are not limited in their applicability to neuroscience. Thus, the presented concepts are suitable for any application generating and processing data and are therefore very well positioned for transfer into different domains.


\begin{figure}
 \includesvg[width=\textwidth]{figures/discussion_global_picture_escus}
 \caption[General schema of scientific data and metadata handling]{General schema of scientific data and metadata handling. Data and metadata generated during an experiment are processed, compiled, integrated and packaged to be deployed to a central data \& metadata repository (center). From this reference data and metadata package scientist can run specific analysis and / or publish the packaged data. The process can be initialized by using a continuous integration system, where it is executed automatically and then be deployed in a readily packaged version to a central repository. By following this process, the data can successively be made sense of and used by a larger community up to a global level, when data are published.}
 \label{fig:disc_general_schema}
\end{figure}



\section{Comparison of Reach-to-Grasp and Vision-for-Action workflows for data and metadata handling}
Since the implementation of the original metadata pipeline conceptually following \citet{Zehl_2016} and published in the Reach-to-Grasp datasets \citep{Brochier_2018} software tools aiding data and metadata handling in neurosciences evolved (see e.g. \cref{sec:metadata,sec:neo}) and shortcomings of the original approach were identified by the use of the data in a large collaborative setting. These changes in tool availability and conceptual differences in the planning and execution of the Vision-for-Action experiment led to different data and metadata approaches in the two projects. We discuss these differences with respect to specific aspects of the experiment in the following.

\subsection{Experimental design}
In the Reach-to-Grasp project systematic metadata collection started during the runtime of the experiment. The lack of a detailed design of a metadata pipeline in preparation for the experiment resulted in metadata being stored in distributed files and various formats, aggravating the systematic collection. To quickly make the most essential metadata accessible with the data, a special loading routine (\code{ReachGraspIO}) was implemented partially containing hard-coded metadata. This decision complicated the systematic metadata aggregation in the long run by introducing circular dependencies in the metadata pipeline set up around the \code{IO}, i.e. hard-coded metadata and metadata of the growing metadata collection co-existed, leading to possible contradictions.
In the Vision-for-Action project we therefore tried to i) minimize the amount of metadata source files, ii) provide them in a consistent, standardized format that is also human readable and user-friendly and iii) refrained from developing a custom loading code that includes data and metadata processing. The \code{csv} format together with additional structural restrictions provides suitable tables which can be easily converted to a hierarchical \software{odML} structure using \software{odMLtables} (\cref{sec:metadata}).
In addition internal changes in the design of the data recording were implemented: With the RIVER setup, special attention went into the integration of the three different types of recording systems. Instead of running the three systems for tracking neuronal, hand and eye data in parallel, these were integrated with the Kinarm system acting as master system for signal integration and coordination. The two Neural Signal Processors serve as the only output streams of the setup by not only writing neuronal, but also eye and hand signals to disc via the two Cerebus systems.
Another improvement implemented in the RIVER setup is the generic encoding of events, which is also used to systematically write parameters and additional metadata into the same files as the neuronal recording data. Storing the complete recording data and metadata in as few files as possible ensures data consistency to a high degree. The introduced generic encoding of events ameliorates the complex event interpretation that was required in the Reach-to-Grasp experiment, where the interpretation of individual events depended on the history of previous events in a complex fashion. With the introduced encoding events have a static interpretation independent of other events. Additionally, they are more robust against errors in the recording as they always consist of a pair of events, forming an information block. Furthermore, the event encoding can be flexibly used for different modes (e.g. task types) of the recording as each task has a unique mapping from these generic to task specific events.

\subsection{Concept for metadata aggregation}
\label{sec:disscussion_metadata_concept}
The concept for compiling a metadata collection differs between the Reach-to-Grasp and the Vision-for-Action experiment (\cref{fig:discussion_comparison_r2g_v4a}). In the former, the metadata structure is defined via a set of templates, which provide an \software{odML} structure containing default values. These templates are generated dynamically by a script on a session-by-session basis and are merged to build a complete template metadata structure. However for generating a suitable template structure information from the metadata sources is already required introducing additional interdependencies in the process and complicating the metadata aggregation (\cref{fig:discussion_comparison_r2g_v4a}, red arrow). In the next step, the default values are replaced with the actual metadata entries extracted from the various source files, which again requires knowledge about the odML structure in a semi hard coded fashion, i.e. template generation and population of the template need to be compatible. Additionally, during the aggregation process the metadata pipeline explicitly attempts to resolve a number of interdependencies between the different metadata sources. This example demonstrates that the intended separation between the structure and content of the metadata collection is not feasible due to dependencies between the two, introducing additional overhead and exception handling in the metadata aggregation procedure.


In the Vision-for-Action project, the metadata aggregation is implemented in a different way: The metadata structure is generated by the same functions that extract the metadata from a source file. This way the metadata content and structure for a specific part of information are handled at the same location in the code, rather than being distributed. This simplifies the metadata aggregation process and allows splitting the process into multiple, independent processes (\cref{fig:discussion_comparison_r2g_v4a}).

\begin{figure}
 \centering
 \includesvg[width=\textwidth, pretex=\relscale{0.8}]{figures/discussion_odml_build_comparison.svg}
 \caption[Comparison of the metadata aggregation for Reach-to-Grasp and Vision-for-Action experiments]{Comparison of the metadata aggregation for Reach-to-Grasp and Vision-for-Action experiments. In the Reach-to-Grasp project, the metadata structure generation is strictly separated from the metadata content (left). The integration of data and metadata occurs in a complex operation combining the two aspects. In the Vision-for-Action project the structure is generated  piece wise during the metadata is aggregation (right). This approach has the advantage that data and metadata are already combined from the beginning and the integration of the individual components of the metadata collection is straight forward (e.g. by using \software{odMLtables}).}
 \label{fig:discussion_comparison_r2g_v4a}
\end{figure}

\subsection{Changes due to software updates}
% odML
The Reach-to-Grasp metadata pipeline was implemented using \software{odML} version 1.3, in which a \software{odML} \code{Properties} can only be generated when containing at least one \code{Value}. This constraint makes the \software{odML} structure as intermediately generated by the Reach-to-Grasp workflow unnecessarily complicated by enforcing the usage of default values for all potential \code{Value} entries. With the release of \software{odML} version 1.4 this constraint was lifted, such that an \software{odML} structure can be created without any \code{Value} entries. Updating the Reach-to-Grasp metadata aggregation pipeline to \software{odML} version 1.4 would therefore simplify the pipeline to a small extend. However, this does not resolve conceptual issues related to the general metadata aggregation concept.

%Nix
Additionally, when the Reach-to-Grasp metadata pipeline was implemented, \software{Neo} did not yet support the \software{Nix} format. For this reason the metadata aggregation pipeline was used to generate a metadata collection in the \software{odML} format, but did not combine data and metadata in a single file. For this purpose, the \code{ReachGraspIO} was implemented to provide a comparable functionality at runtime. However, this introduced additional interdependencies within the project (\cref{sec:disscussion_metadata_concept}) and attenuated the user-friendliness by requiring the usage of custom code. This situation where code, metadata and data are always required to be fully consistent on the one side, but on the other side the metadata is continuously developed and extended leads to frequent inconsistencies on the user side. This thwarts the implementation of analysis while still using an up-to-date version of data and metadata.


\subsection{Usability}
The Reach-to-Grasp pipeline generates a single \software{odML} file per recording session. Therefore accessing the data as well as the metadata requires the user to have  specific, compatible versions of data and metadata files and software packages set up. This includes the metadata in \software{odML} format, the data distributed across one \code{nev}, \code{ns2} and \code{ns6} file for each recording session. In addition, the \software{Neo} and \software{odML} Python packages, as well as the custom \code{ReachGraspIO}, is required for accessing all information contained in the data and metadata files. This, however, only provides a basic annotation of the data with some selected content from the metadata collection and does not provide direct linking between data and metadata structures. If the users require additional metadata beyond the annotations, they need to find and extract this information manually from the metadata collection.

Applying the same strategy in the Vision-for-Action project would have resulted in several metadata source files and also two sets of neuronal data files, as the setup contains two NSPs. Therefore, here the data and metadata workflow generates a single \software{Nix} file combining data and metadata in a singular framework, in which the data and basic metadata are accessible requiring only the Python \software{Neo} package. For accessing additional metadata any commonly available \code{hdf5} viewer can be used. A project in development for visualization of \software{Nix} files taking into account basic metadata is \software{NixView}\footnote{NixView, \url{https://github.com/bendalab/nixview}}. Combining data and metadata in a single file guarantees the correspondence between the two, whereas in the Reach-to-Grasp project this needs to asserted manually.

\subsection{Pipeline and workflow approach}
Within the Reach-to-Grasp project, a single Python script orchestrates the generation of the \software{odML} structure and enrichment with metadata. This results in convoluted code as one tries to separate the process of building and reading the \software{odML} structure and metadata sources with moderate success. In the Vision-for-Action project this separation of creating templates and populating them is actively avoided resulting in a much more flexible, reusable and scalable workflow consisting of modular steps interacting only via their in- and output files. In addition, the workflow is easier to understand as the dependencies can be automatically visualized and the workflow can naturally be executed piece wise, which aids troubleshooting and exploration. Furthermore, the workflow steps can be separated into generic and experiment specific components, where the former provide a basis for the exchange and sharing of metadata workflow approaches across projects and laboratories. This automatically makes data and metadata handling more comparable and therefore provides a foundation for exchange and publication of scientific data. For this reason, we abstracted a set of general guidelines for the handling of data and metadata from our experiences, which are described in \cref{sec:guidelines}.


\section{Outlook}
\subsection{The future of \software{odMLtables}}
\software{odMLtables} emerged from a collection of \software{odML} utility functions that accumulated in the context of the Reach-to-Grasp project. Currently, it is a standalone Python package with key dependencies on \software{odML} and \software{PyQt5}. The graphical user interface (gui) provides non-programmatic access to the core features of \software{odMLtables}. For historic reasons, \software{odMLtables} internally uses a custom, dictionary-based representation of the \software{odML} structure. Replacing this with a native \software{odML} Python object will ensure consistency during the metadata manipulation using \software{odMLtables}.

In addition to \software{odMLtables}, a native \software{odML} editor, \software{odML-UI}, exists, which only operates on the hierarchical \software{odML} representation. Currently the four main features of the \software{odMLtables} gui can be accessed via \software{odML-UI} if \software{odMLtables} is installed. However, in the long run, it would be most user-friendly to integrate the two tools into a single one thereby reducing the dependencies on the user side and providing a more concise set of tools for metadata handling. With an enhanced \software{odMLtables} version using a Python \software{odML} representation internally, integration of the two tools will be straightforward.

The metadata structure used within the \software{Nix} model is based on \software{odML} and metadata can easily exported into and read from an \software{odML} file via the \software{nix-odML-converter}\footnote{nix-odML-converter, \url{https://pypi.org/project/nixodmlconverter}}. This permits the straight forward integration of \software{Nix} as an additional file format supported by \software{odMLtables} by utilizing the \software{nix-odML-converter}.

\subsection{The future of \software{Neo}}
We described the evolution of the \software{Neo} package from the original publication \cite{Garcia_2014} to the current version $0.7.2$ as well as potential enhancements in future versions in \cref{sec:neo}. The basic concepts for capturing data within \software{Neo} objects are rather stable and also the \software{Neo} container objects for handling temporal relations between these data objects (\code{Segments}) did not change in the last releases. In contrast to that, the container objects for capturing channel and general object relations was updated frequently. In the current \software{Neo} version, this is implemented by \code{ChannelIndex} objects covering a number of different functionalities for handling object relations. We suggest splitting these different functionalities to create a set of a few simple objects and methods fulfilling the same task. The first step of this was already implemented in the form of array annotations. In the next steps we suggest to introduce \code{Group} and \code{View} objects (\cref{sec:neo_outlook}). However, this suggestion is still under debate and will if at all only be adjusted in the future. 

With the release of \software{Neo} version $0.6$ a standardized API for readers was introduced. This harmonized the multiple implementation approaches collected in the \software{Neo} framework and at the same time improved the performance of the readers. However, this only affects the reading aspect of \code{Neo}. On the writing side, there is no standardization of code, since the writing to multiple formats requires more diverse code organization than funneling different file formats into a single representation. Additionally,  the IOs writing capabilities are limited to eight implementations.  Thus, the effort of finding a code structure suited for general writing must take that into account. However, two useful extensions on the writing side will be i) a validator, checking the integrity before writing the \software{Neo} structure to disc and forming a standard of valid and writable \software{Neo} structures and ii) unittests for ensuring the compatibility between writable and readable \software{Neo} structures (see \cref{fig:disc_neo_plans}).

\subsection{Automated workflow management}
With the pilot study investigating the integration of \software{snakemake} workflows in the \software{Gin} web service\footnote{gin-proc, \url{https://github.com/G-Node/gin-proc}} the development takes a direction towards a fully automated data and metadata workflow. Here the creation of new recording data files triggers the workflow annotating, preprocessing and preparing these data in a version controlled manner for scientific usage. This permits the setup of a workflow based on continuous integration and deployment principles, automatically executing the workflow upon a change in the source files and updating the packaged data stored at a central, versioned location. Here the setup of a system capable of dealing with large datasets as generated by electrophysiological experiments and the sustainable storage of past versions of these large data are issues to be solved before the system can be used in a scientific application.

\begin{figure}
 \includesvg[width=\textwidth]{figures/discussion_neo.svg}
 \caption[\software{Neo} IOs and future plans]{\software{Neo} IOs and future plans. \software{Neo} supports reading of multiple file formats. A large fraction of these formats is read via standardized implementations of readers whereas other rely on format-specific, custom implementations. In addition, \software{Neo} can write to a number of formats with custom writers. We suggest to extend the current implementation with a \code{validation} mechanism as well as a systematic \code{unittest} approach for formats that are read- and writable which ensures coherent reading and writing functionality (dashed boxes).}
 \label{fig:disc_neo_plans}
\end{figure}

\subsection{Data analysis}
For analysis of electrophysiological data, there are a number of tools available with different analysis focuses (e.g. see \citet{Unakafova_2019}). Since the data and metadata workflow presented in \cref{sec:workflows} creates a comprehensive data representation in the \code{hdf5} format, in principle any of these toolboxes can be used for analysis as long as \code{hdf5} reading capability is available in the corresponding programming language. However, analysing the data using a \software{Neo} based approach provides a direct and simple access to the data. This leaves mainly four tools for data analysis based on \software{Neo}: \software{Tridesclous}\footnote{Tridesclous, \url{https://tridesclous.readthedocs.io}} for spike sorting, \software{Open Electrophy}\footnote{Open Electrophy, \url{https://pypi.org/project/OpenElectrophy/}} \citep{Garcia_2009} for viewing and explorative analysis, \software{SpykeViewer}\footnote{SpykeViewer, \url{https://pypi.org/project/spykeviewer/}} for navigation, analysis and visualization and \software{Elephant} for comprehensive data analysis. Of course the data can also be extracted from the \software{Neo} structure and used in any other Python-based analysis tool, however, this forfeits the inherent data consistency and metadata annotations. Therefore \software{Elephant} is the tool of choice here. It offers a wide range of basic and a number of advanced methods for the analysis of spiking and continuous neural signal activity. As \software{Elephant} is a community-driven open-source toolkit also extensions are highly welcome.

Any type of analysis can be supported also from the \software{Neo} side. Here extending the set of utility functionality, e.g. for data selection and preparation for the analysis would benefit the user independent of the specific analysis toolbox used. Two examples for potential features to be extended and improved on the \software{Neo} side are i) the filter functionality for efficient selection of data objects based on their attributes and custom annotations ii) the utility functions for user-friendly interaction with memory-optimized \software{Neo} structures (lazy objects).

\subsection{Published datasets}
The two electrophysiological datasets described in \citep{Brochier_2018} and published in April 2018 have not been reused in an independent study. This might be due to the fact, that even though the dataset is very rich, is also limited to a single cortical area and a monkey performing a very specific task. Therefore the dataset might be of scientific interest for two types of scientists: i) researchers investigating very similar questions to those addressed in the experiment or that require parallel recordings on 96 electrodes or ii) researchers looking for a generic example datasets, e.g. to develop new workflows or test new processing and analysis methods. For these, this dataset might not be visible enough or the extensive description dissuasive, as typically much less metadata is required for these purposes. In these cases, the indexing of the dataset by a more general data catalogue will increase the discovery chances of the dataset, e.g. via the Google dataset search\footnote{Google dataset search, \url{https://toolbox.google.com/datasetsearch}}.
However, the publication of the dataset has paid off as the community now has a publicly available example of the type of data we deal with. The high number of citations\footnote{number of citations of \citep{Brochier_2018} on 23rd of August 2019 (1 year and 4.5 months after publication) according to PubMed\footnote{\url{https://www.ncbi.nlm.nih.gov/pubmed?linkname=pubmed_pubmed_citedin&from_uid=29633986}}: 6} further demonstrates the importance and value of doing the effort to publish data. 
Additionally these datasets have been frequently used for teaching purposes, e.g. in tutorial Jupyter notebooks introducing \software{snakemake} workflows, \software{Neo} and or \software{Elephant}, e.g. at the University of Toronto\footnote{\url{https://github.com/UofTCoders/Events/issues/239} and \url{https://github.com/UofTCoders/studyGroup/tree/gh-pages/lessons/python/snakemake_elephant_demo}}.

\subsection{Lessons to learn}
As already hinted at in the future challenges for the Vision-for-Action workflow (\cref{sec:workflow_discussion}) and the guidelines formulated in \cref{sec:guidelines}, we applied a couple of concepts from professional software development in the in the workflow implementation. Some concepts which can be adapted for example for scientific software development from agile software development are pair programming, continuous integration, short feedback loops and continuous deployment \citep{Shore_2007}. We already described in detail the integration of continuous integration and deployment in the presented data and metadata workflow. The pair programming technique for joint coding involves two programmers working in a team on a single computer. This technique promotes creative approaches as well as code review during the implementation process. These features are also of advantage for scientific programming and can therefore be easily adopted. The concept of short feedback loops focuses on frequent interaction with the customer / user of the software product to get immediate feedback and quickly adapt to customer needs. In case of the presented workflows, the users would be the scientists using the packaged data and metadata for analysis providing feedback for the implemented features of the data packages. However, recognizing and implementing these concepts requires practice and organization, meaning that the potential to learn from the software development community is great, but it takes time and initiative to be implemented in a scientific environment.

\subsection{Concept extension}
On the software side, the described workflow was developed for the specific research area of neurophysiology by dealing with electrophysiological data from a monkey experiment. However, in principle, the tools presented are capable of dealing with different measurement modalities. For example the description of an electroencephalographic (EEG), functional magnetic resonance imaging (fMRI) dataset or a spiking network simulation (e.g. using Nest\footnote{Nest, \url{https://www.nest-simulator.org/}, RRID:SCR\_002963, doi:10.5281/zenodo.2605421}) should be possible using similar means as presented here as the boundaries between the scientific areas are fuzzy. This will permit to easily apply the same analysis methods on datasets e.g. on spiking activity from calcium imaging data and sharp electrodes recordings and will, therefore, form a bridge between the different areas of neuroscience.
On a larger scale, the development of concepts and tools for data and metadata management across scientific disciplines is an area of active development \cite{Amari_2002, Cheung_2009, Nichols_2015}. Here our efforts to provide a generic tool set located within the field of neuroscience provides a foundation for the integration with other fields of science.

\subsection{Dissemination of a data and metadata workflow system}
The implementation of the data and metadata workflow is a natural precursor to publishing the datasets, such that the research community can benefit from the invested effort. Such a workflow typically covers four aspects of data preparation (see also \cref{fig:v4a_metadata_workflow_rulegraph}): preprocessing of the data, aggregation of metadata, packaging of the data and metadata and the versioning and deployment of the packages. The former requires data and domain-specific processing steps, as the preparation of the data for analysis is highly specific to the type of data. Here, the implementation of custom code or integration of tools in a scripted fashion can not be avoided. For the second step, the aggregation of metadata, the requires effort highly depends on the type of source files generated during the experiment. In the ideal case of a comprehensive set of metadata source files in a standardized format, the aggregation of the metadata collection can be performed completely based on generic workflow steps (e.g. as the ones described in \ref{sec:workflows}). In the ideal case these workflow steps are available publicly in the form of a collection of generic workflow utility components. Packaging the data together with the metadata again highly depends on the standardization of the formats used in previous steps. For a standardized formats utility functionality for easy integration of data and metadata should be available. The versioning and deployment steps of the workflow are independent of the scientific discipline and standards used. The automatic deployment of any dataset to a commonly used data repository or hosting platform should be available as a generic component of the workflow management system. In summary, the degree of custom implementation effort for a given project highly depends on the number of standardized tools and formats used within the project. In the ideal case of a very standardized set of tools and formats, the effort for setting up a data and metadata pipeline is minimal and consists mostly of the implementation of suitable preprocessing steps for a particular type of data. 

For the successful dissemination of the workflow concept based on \software{snakemake}  as presented here in the field of neuroscience, the establishment of a public collection of snakemake utility functions is essential. These functions will provide the building blocks for setting up data and metadata workflows. By collecting generic rules based on tools with standardized interfaces and formats, both, the tools and the community will benefit.  The community grows, usability will avail from extended standardization of data and metadata and finally, further facilitation of data publication and sharing will occur.


\paragraph{Extensions}
The concept of workflow management is not limited to the application in data and metadata management but can be applied e.g. also for data analysis and publication as well as data acquisition.
Here potential extensions towards the data acquisition can include the integration of the data acquisition system in the continuous integration system and data workflow. This would permit the prompt execution of preprocessing steps upon data recording, providing immediate feedback to the experimenter about the data quality and potential issues in the recorded data. In the field of electrophysiology a project promoting open hardware and software solutions is Open Ephys\footnote{OpenEphys, \url{http://www.open-ephys.org/}} \citep{Siegle_2017}. This project provides modular open-source hardware for tools for data acquisition and experiment control as well as accompanying software solutions for data acquisition and visualization.

For metadata acquisition besides the presented solution utilizing \code{csv} files in combination with \software{odMLtables}, also the integration of electronic labnotebooks (ELNs) is a promising extension to data and metadata workflows in experimental laboratories. This would be one solution to automatize the metadata integration into the workflow as discussed above. The advantage of ELNs is the automatic standardization of metadata within the system of the ELN. Using a workflow management system, this information can be accessed and potentially converted into other standardized metadata formats, e.g. \software{odML}. One potential tool to perform such a conversion is \software{odMLtables}, it can act as a bridge between \software{odML} and ELNs as many of these also support a tabular metadata representation (see \cref{sec:Discussion}).

We discussed the usage of \software{Gin} as versioning and deployment service in the presented workflow (\ref{sec:workflow_discussion}). Here, also other methods for providing the packaged data to collaborators and the scientific community is possible, e.g. the registration of the dataset in a central database or a custom repository. Here, for example, the \software{Gin} service can be installed on a local server, e.g. in case of infrastructure limitation or data privacy restrictions. This would permit to use the same workflow as presented, only the remote server location and authentication needs to be adapted. 

In medical applications typically recorded data are anonymised using a standardized procedure of assigning unique ids to datasets and removing human or machine readable information that allows conclusions about the identity of the patient. This anonymization process can be easily integrated into a workflow by adding a rule for the modification of the data files which can optionally track the patient-identifier mapping in a separate file only accessible to authorized personnel. In case of different levels of authorizations, the data can be anonymized to different degrees and then copied to locations only accessible by personnel with a certain authorization level. This way the anonymization procedure would be robust to human errors and could be even automatized to a degree that no human has access to the patient-identity mapping.

% \paragraph{Limitations}
% Workflows are a well suited methods for automating repetetive procedures. However, as soon as frequent manual interaction is required during the workflow the presented tools reach their limit.


\subsection{Looking further ahead}
A large part of the efforts presented in this manuscript arose from the fact, that data generated by commercial recording systems were not complete and free of unintended signals (artifacts). This lead to two types of development: the aggregation of as much information as possible concerning the recording circumstances and the development of extensive preprocessing steps including artifact detection. The question is: Is this really necessary and will it always be like this?

For the first aspect, this is diligent work requiring first of all commercial recording setup producers to take this aspect into consideration and extend and adjust their systems according to the needs of scientist to comprehensively track the data generation. For custom build setups or integrated systems as in the case of Vision-for-Action it will still be in the scope of duties of the scientist to be aware of this issue and tackle it early in the experiment development process.

The second aspect of artifacts in recording data is an issue typically improving with technical development, e.g. better insulation of cables or less error-prone communication protocols. However, no experimental setup will ever be completely free of artifacts and also a change to the latest technology will never guarantee unintended side effects on the data. Therefore careful quality checks of recording data will always be necessary, only the amount of contaminated data might reduce with technical progress.

For these reasons workflows like the one presented in \cref{sec:workflows} deal with an essential aspect of scientific research and are the key to well-founded scientific findings.

For these reasons workflows like the one presented in cref{sec:workflows} deal with a fundamental aspect of contemporary scientific research.  Therefore, they should be considered as a key to well-founded scientific findings, and thus may be a key contributor to an expanding field of data science in general.
