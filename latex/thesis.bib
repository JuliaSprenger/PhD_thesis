
@article{Ram_2013,
	title = {Git can facilitate greater reproducibility and increased transparency in science},
	volume = {8},
	issn = {1751-0473},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3639880/},
	doi = {10.1186/1751-0473-8-7},
	abstract = {Background
Reproducibility is the hallmark of good science. Maintaining a high degree of transparency in scientific reporting is essential not just for gaining trust and credibility within the scientific community but also for facilitating the development of new ideas. Sharing data and computer code associated with publications is becoming increasingly common, motivated partly in response to data deposition requirements from journals and mandates from funders. Despite this increase in transparency, it is still difficult to reproduce or build upon the findings of most scientific publications without access to a more complete workflow.

Findings
Version control systems (VCS), which have long been used to maintain code repositories in the software industry, are now finding new applications in science. One such open source VCS, Git, provides a lightweight yet robust framework that is ideal for managing the full suite of research outputs such as datasets, statistical code, figures, lab notes, and manuscripts. For individual researchers, Git provides a powerful way to track and compare versions, retrace errors, explore new approaches in a structured manner, while maintaining a full audit trail. For larger collaborative efforts, Git and Git hosting services make it possible for everyone to work asynchronously and merge their contributions at any time, all the while maintaining a complete authorship trail. In this paper I provide an overview of Git along with use-cases that highlight how this tool can be leveraged to make science more reproducible and transparent, foster new collaborations, and support novel uses.},
	urldate = {2015-10-29},
	journal = {Source Code Biol Med},
	author = {Ram, Karthik},
	month = feb,
	year = {2013},
	pmid = {23448176},
	pmcid = {PMC3639880},
	pages = {7},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/PZR76Z8B/Ram - 2013 - Git can facilitate greater reproducibility and inc.pdf:application/pdf}
}

@article{Eglen_2017,
	title = {Toward standard practices for sharing computer code and programs in neuroscience},
	volume = {20},
	copyright = {{\textcopyright} 2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1097-6256},
	url = {https://www.nature.com/neuro/journal/v20/n6/full/nn.4550.html},
	doi = {10.1038/nn.4550},
	abstract = {Computational techniques are central in many areas of neuroscience and are relatively easy to share. This paper describes why computer programs underlying scientific publications should be shared and lists simple steps for sharing. Together with ongoing efforts in data sharing, this should aid reproducibility of research.},
	language = {en},
	number = {6},
	urldate = {2017-07-10},
	journal = {Nat Neurosci},
	author = {Eglen, Stephen J. and Marwick, Ben and Halchenko, Yaroslav O. and Hanke, Michael and Sufi, Shoaib and Gleeson, Padraig and Silver, R. Angus and Davison, Andrew P. and Lanyon, Linda and Abrams, Mathew and Wachtler, Thomas and Willshaw, David J. and Pouzat, Christophe and Poline, Jean-Baptiste},
	month = jun,
	year = {2017},
	keywords = {Neuroscience, Scientific community},
	pages = {770--773},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/3678DAJA/Eglen et al. - 2017 - Toward standard practices for sharing computer cod.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/UEVCKQBZ/nn.4550.html:text/html;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/IH5KZT3K/317303165_Toward_standard_practices_for_sharing_computer_code_and_programs_in_neuroscience.pdf:application/pdf}
}

@article{Plesser_2018,
	title = {Reproducibility vs. {Replicability}: {A} {Brief} {History} of a {Confused} {Terminology}},
	volume = {11},
	issn = {1662-5196},
	shorttitle = {Reproducibility vs. {Replicability}},
	url = {https://www.frontiersin.org/articles/10.3389/fninf.2017.00076/full},
	doi = {10.3389/fninf.2017.00076},
	abstract = {Reproducibility vs. Replicability: A Brief History of a Confused Terminology},
	language = {English},
	urldate = {2018-06-13},
	journal = {Front. Neuroinform.},
	author = {Plesser, Hans E.},
	year = {2018},
	keywords = {reproducibility, artifacts, computational science, repeatability, replicability},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/6MVUPPIH/Plesser - 2018 - Reproducibility vs. Replicability A Brief History.pdf:application/pdf}
}

@article{Milkowski_2018,
	title = {Replicability or reproducibility? {On} the replication crisis in computational neuroscience and sharing only relevant detail},
	volume = {45},
	issn = {0929-5313, 1573-6873},
	shorttitle = {Replicability or reproducibility?},
	url = {http://link.springer.com/10.1007/s10827-018-0702-z},
	doi = {10.1007/s10827-018-0702-z},
	abstract = {Replicability and reproducibility of computational models has been somewhat understudied by Bthe replication movement.{\textasciicircum} In this paper, we draw on methodological studies into the replicability of psychological experiments and on the mechanistic account of explanation to analyze the functions of model replications and model reproductions in computational neuroscience. We contend that model replicability, or independent researchers' ability to obtain the same output using original code and data, and model reproducibility, or independent researchers' ability to recreate a model without original code, serve different functions and fail for different reasons. This means that measures designed to improve model replicability may not enhance (and, in some cases, may actually damage) model reproducibility. We claim that although both are undesirable, low model reproducibility poses more of a threat to long-term scientific progress than low model replicability. In our opinion, low model reproducibility stems mostly from authors' omitting to provide crucial information in scientific papers and we stress that sharing all computer code and data is not a solution. Reports of computational studies should remain selective and include all and only relevant bits of code.},
	language = {en},
	number = {3},
	urldate = {2019-01-11},
	journal = {Journal of Computational Neuroscience},
	author = {Mi{\l }kowski, Marcin and Hensel, Witold M. and Hohol, Mateusz},
	month = dec,
	year = {2018},
	pages = {163--172},
	file = {Mi{\l }kowski et al. - 2018 - Replicability or reproducibility On the replicati.pdf:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/BVDSTLVI/Mi{\l }kowski et al. - 2018 - Replicability or reproducibility On the replicati.pdf:application/pdf}
}

@book{Zehl_2018,
	address = {J{\"u}lich},
	series = {Schriften des {Forschungszentrums} {J{\"u}lich} {Reihe} {Schl{\"u}sseltechnologien} / {Key} {Technologies}},
	title = {Management of {Electrophysiological} {Data} \& {Metadata} - {Making} complex experiments accessible to yourself and others},
	isbn = {978-3-95806-311-2},
	abstract = {As neuroscientists, we are obligated to guarantee the reliability of our research workflows and results.For this reason, most neuroscientific, peer-reviewed journals require, besides a full description of thenew scientific findings and used methods, at least also a brief summary of the used data. For a completelyopen scientific inquiry and to further promote scientific progress as required by most nationalfunding agencies, it would be best to share the raw data along with an analysis paper or independentlyas a standalone data publication. Unfortunately, especially the neuroscientific community ishesitant to share own research data with third parties, because guidelines, tools and support for thepublishing authors to provide data and corresponding adequate information on the experiment aregenerally hard to formalize and often missing. As a consequence, published scientific results remainunreproducible by other researchers. Along an example of a complex electrophysiological experimentwhich was conducted by external collaboration partners, I will demonstrate how to share and publishdata, but also identify the reasons why researchers, in particular experimentalists in electrophysiology,are hesitant to try the same. For this, I first provide a data descriptor of the experiment following theguidelines of a pure data journal from the Nature publishing group, called Scientific Data. Accordingto the journal guidelines, I describe all information necessary to be able to understand the setupand workflow of the experiment as well as the minimum information necessary to be able to workwith the corresponding datasets. The latter requires the provision of a robust data loading routine.To guarantee the access to the data of the experiment, I implemented a commonly usable loadingroutine for the data formats of the used data acquisition (DAQ) system from Blackrock Microsystems(Cerebus DAQ), and published it as part of an open source data framework, called Neo. Neo hasthe advantage of representing data in standardized structures that allow researchers to use commonanalysis routines on different data formats. In addition, it is possible to further annotate the Neo datastructures with experiment-specific information on the data. To automatically integrate such information,termed metadata, it is best to have them organised in a machine-readable format. Althoughseveral software solutions for such metadata formats exist, they are usually not tested for complexuse cases, such as the example experiment. In most cases, they only provide the framework itself as astandardized metadata representation or specification, and no solutions for how to actually compile auseful metadata collection. For the example experiment, I chose a metadata framework, called openmetadata Markup Language (odML), an open source project developed by the German Node (GNode)of the International Neuroinformatics Coordination Facility (INCF). In the second part of thethesis, I demonstrate how to organize metadata for the experiment, to be able to compile and use acorresponding odML metadata collection. To facilitate the compilation process for my collaborators, Ideveloped a Python package, called odMLtables, which facilitates the access to the odML frameworkby an algorithmic transformation of odML into a spreadsheet format (csv or xls) and back. In addition,I provided a complete workflow for collecting and storing the metadata of the experiment intoa comprehensive odML-file collection. Furthermore, I provided a specified data loading routine thatautomatically annotates the data structures with the corresponding metadata of the collection. Thelatter improves the workflow in the course of neuroscientific analyses of the data from the exampleexperiment, as demonstrated in the last part of my thesis. In summary, I show that the preparationsto properly share research data within a scientific collaboration are cumbersome and time consuming,but essential for successfully publishing data and analysis results for a broader audience of users. Topromote data sharing within the neuroscientific community and to provide a better foundation forreproducible research, my thesis offers a coherent strategy for managing electrophysiological data andmetadata using a well selected set of available technologies},
	number = {167},
	publisher = {Forschungszentrum J{\"u}lich GmbH Zentralbibliothek, Verlag},
	author = {Zehl, Lyuba},
	year = {2018}
}

@article{Zhang_2017,
	title = {{NeoAnalysis}: a {Python}-based toolbox for quick electrophysiological data processing and analysis},
	volume = {16},
	issn = {1475-925X},
	shorttitle = {{NeoAnalysis}},
	url = {https://doi.org/10.1186/s12938-017-0419-7},
	doi = {10.1186/s12938-017-0419-7},
	abstract = {In a typical electrophysiological experiment, especially one that includes studying animal behavior, the data collected normally contain spikes, local field potentials, behavioral responses and other associated data. In order to obtain informative results, the data must be analyzed simultaneously with the experimental settings. However, most open-source toolboxes currently available for data analysis were developed to handle only a portion of the data and did not take into account the sorting of experimental conditions. Additionally, these toolboxes require that the input data be in a specific format, which can be inconvenient to users. Therefore, the development of a highly integrated toolbox that can process multiple types of data regardless of input data format and perform basic analysis for general electrophysiological experiments is incredibly useful.},
	number = {1},
	urldate = {2019-07-04},
	journal = {BioMedical Engineering OnLine},
	author = {Zhang, Bo and Dai, Ji and Zhang, Tao},
	month = nov,
	year = {2017},
	pages = {129},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/CSYX3XVF/Zhang et al. - 2017 - NeoAnalysis a Python-based toolbox for quick elec.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/X4UT4Q3H/s12938-017-0419-7.html:text/html}
}

@article{vanderWalt_2011,
	title = {The {NumPy} {Array}: {A} {Structure} for {Efficient} {Numerical} {Computation}},
	volume = {13},
	issn = {1521-9615},
	shorttitle = {The {NumPy} {Array}},
	url = {http://ieeexplore.ieee.org/document/5725236/},
	doi = {10.1109/MCSE.2011.37},
	number = {2},
	urldate = {2019-07-09},
	journal = {Comput. Sci. Eng.},
	author = {van der Walt, St{\'e}fan and Colbert, S Chris and Varoquaux, Ga{\"e}l},
	month = mar,
	year = {2011},
	pages = {22--30},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/SN67SLJS/van der Walt et al. - 2011 - The NumPy Array A Structure for Efficient Numeric.pdf:application/pdf}
}

@article{Perez_2007,
	title = {{IPython}: {A} {System} for {Interactive} {Scientific} {Computing}},
	volume = {9},
	issn = {1521-9615},
	shorttitle = {{IPython}},
	url = {http://ieeexplore.ieee.org/document/4160251/},
	doi = {10.1109/MCSE.2007.53},
	number = {3},
	urldate = {2019-07-09},
	journal = {Comput. Sci. Eng.},
	author = {Perez, Fernando and Granger, Brian E.},
	year = {2007},
	pages = {21--29}
}

@book{Jones_2001,
	title = {{SciPy}: {Open} source scientific tools for {Python}},
	url = {http://www.scipy.org/},
	author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and {others}},
	year = {2001}
}

@misc{Dale_,
	title = {Quantities {Documenting}},
	url = {https://python-quantities.readthedocs.io/en/latest/devel/documenting.html},
	urldate = {2019-07-09},
	author = {Dale, Darren},
	file = {Documenting Quantities {\textemdash} quantities 0.12.1+6.g1ef120b documentation:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/CDEK8USI/documenting.html:text/html}
}

@article{Kelly_2007,
	title = {Comparison of recordings from microelectrode arrays and single electrodes in visual cortex},
	volume = {27},
	issn = {0270-6474},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3039847/},
	doi = {10.1523/JNEUROSCI.4906-06.2007},
	number = {2},
	urldate = {2019-07-19},
	journal = {J Neurosci},
	author = {Kelly, Ryan C. and Smith, Matthew A. and Samonds, Jason M. and Kohn, Adam and Bonds, A.B. and Movshon, J. Anthony and Lee, Tai Sing},
	month = jan,
	year = {2007},
	pmid = {17215384},
	pmcid = {PMC3039847},
	pages = {261--264},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/A5Y3NEQH/Kelly et al. - 2007 - Comparison of recordings from microelectrode array.pdf:application/pdf}
}

@article{Shew_2010,
	title = {Simultaneous multi-electrode array recording and two-photon calcium imaging of neural activity},
	volume = {192},
	issn = {0165-0270},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2934901/},
	doi = {10.1016/j.jneumeth.2010.07.023},
	abstract = {A complete understanding of how brain circuits function will require measurement techniques which monitor large scale network activity simultaneously with the activity of local neural populations at a small scale. Here we present a useful step towards achieving this aim: simultaneous two-photon calcium imaging and multi-electrode array (MEA) recordings. The primary challenge of this method is removing an electrical artifact from the MEA signals that is caused by the imaging laser. Here we show that artifact removal can be achieved with a simple filtering scheme. As a demonstration of this technique we compare large-scale local field potential signals to single-neuron activity in a small-scale group of cells recorded from rat acute slices under two conditions: suppressed vs. intact inhibitory interactions between neurons.},
	number = {1},
	urldate = {2019-07-19},
	journal = {J Neurosci Methods},
	author = {Shew, Woodrow L. and Bellay, Timothy and Plenz, Dietmar},
	month = sep,
	year = {2010},
	pmid = {20659501},
	pmcid = {PMC2934901},
	pages = {75--82},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/U5MZ34FK/Shew et al. - 2010 - Simultaneous multi-electrode array recording and t.pdf:application/pdf}
}

@article{Giovannucci_2019,
	title = {{CaImAn} an open source tool for scalable calcium imaging data analysis},
	volume = {8},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.38173},
	doi = {10.7554/eLife.38173},
	abstract = {Advances in fluorescence microscopy enable monitoring larger brain areas in-vivo with finer time resolution. The resulting data rates require reproducible analysis pipelines that are reliable, fully automated, and scalable to datasets generated over the course of months. We present CaImAn, an open-source library for calcium imaging data analysis. CaImAn provides automatic and scalable methods to address problems common to pre-processing, including motion correction, neural activity identification, and registration across different sessions of data collection. It does this while requiring minimal user intervention, with good scalability on computers ranging from laptops to high-performance computing clusters. CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. To benchmark the performance of CaImAn we collected and combined a corpus of manual annotations from multiple labelers on nine mouse two-photon datasets. We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons.},
	urldate = {2019-07-19},
	journal = {eLife},
	author = {Giovannucci, Andrea and Friedrich, Johannes and Gunn, Pat and Kalfon, J{\'e}r{\'e}mie and Brown, Brandon L and Koay, Sue Ann and Taxidis, Jiannis and Najafi, Farzaneh and Gauthier, Jeffrey L and Zhou, Pengcheng and Khakh, Baljit S and Tank, David W and Chklovskii, Dmitri B and Pnevmatikakis, Eftychios A},
	editor = {Kleinfeld, David and King, Andrew J},
	month = jan,
	year = {2019},
	keywords = {calcium imaging, software, data analysis, one-photon, open source, two-photon},
	pages = {e38173},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/2NMA264S/Giovannucci et al. - 2019 - CaImAn an open source tool for scalable calcium im.pdf:application/pdf}
}

@article{Hunter_2007,
	title = {Matplotlib: {A} 2D graphics environment},
	volume = {9},
	doi = {10.1109/MCSE.2007.55},
	abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, J. D.},
	year = {2007},
	pages = {90--95}
}

@article{Ray_2016,
	title = {{NSDF}: {Neuroscience} {Simulation} {Data} {Format}},
	volume = {14},
	issn = {1559-0089},
	shorttitle = {{NSDF}},
	url = {https://doi.org/10.1007/s12021-015-9282-5},
	doi = {10.1007/s12021-015-9282-5},
	abstract = {Data interchange is emerging as an essential aspect of modern neuroscience. In the areas of computational neuroscience and systems biology there are multiple model definition formats, which have contributed strongly to the development of an ecosystem of simulation and analysis tools. Here we report the development of the Neuroscience Simulation Data Format (NSDF) which extends this ecosystem to the data generated in simulations. NSDF is designed to store simulator output across scales: from multiscale chemical and electrical signaling models, to detailed single-neuron and network models, to abstract neural nets. It is self-documenting, efficient, modular, and scalable, both in terms of novel data types and in terms of data volume. NSDF is simulator-independent, and can be used by a range of standalone analysis and visualization tools. It may also be used to store variety of experimental data. NSDF is based on the widely used HDF5 (Hierarchical Data Format 5) specification and is open, platform-independent, and portable.},
	language = {en},
	number = {2},
	urldate = {2019-07-29},
	journal = {Neuroinform},
	author = {Ray, Subhasis and Chintaluri, Chaitanya and Bhalla, Upinder S. and W{\'o}jcik, Daniel K.},
	month = apr,
	year = {2016},
	keywords = {Analysis tools, Data format, Data sharing, Simulations, Visualization},
	pages = {147--167},
	file = {Springer Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/8F3DJAGM/Ray et al. - 2016 - NSDF Neuroscience Simulation Data Format.pdf:application/pdf}
}

@article{Hazan_2006,
	title = {Klusters, {NeuroScope}, {NDManager}: {A} free software suite for neurophysiological data processing and visualization},
	volume = {155},
	issn = {0165-0270},
	shorttitle = {Klusters, {NeuroScope}, {NDManager}},
	url = {http://www.sciencedirect.com/science/article/pii/S0165027006000410},
	doi = {10.1016/j.jneumeth.2006.01.017},
	abstract = {Recent technological advances now allow for simultaneous recording of large populations of anatomically distributed neurons in behaving animals. The free software package described here was designed to help neurophysiologists process and view recorded data in an efficient and user-friendly manner. This package consists of several well-integrated applications, including NeuroScope (http://neuroscope.sourceforge.net), an advanced viewer for electrophysiological and behavioral data with limited editing capabilities, Klusters (http://klusters.sourceforge.net), a graphical cluster cutting application for manual and semi-automatic spike sorting, NDManager (http://ndmanager.sourceforge.net), an experimental parameter and data processing manager. All of these programs are distributed under the GNU General Public License (GPL, see http://www.gnu.org/licenses/gpl.html), which gives its users legal permission to copy, distribute and/or modify the software. Also included are extensive user manuals and sample data, as well as source code and documentation.},
	number = {2},
	urldate = {2019-07-29},
	journal = {Journal of Neuroscience Methods},
	author = {Hazan, Lynn and Zugaro, Micha{\"e}l and Buzs{\'a}ki, Gy{\"o}rgy},
	month = sep,
	year = {2006},
	keywords = {Spike sorting, Cluster cutting, Data mining, Local field potentials, Unit activity},
	pages = {207--216},
	file = {ScienceDirect Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/XIVLMVJL/Hazan et al. - 2006 - Klusters, NeuroScope, NDManager A free software s.pdf:application/pdf;ScienceDirect Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/4IBZIWS9/S0165027006000410.html:text/html}
}

@misc{TheHDFGroup_1997,
	title = {Hierarchical {Data} {Format}, version 5},
	author = {{The HDF Group}},
	year = {1997}
}

@article{Teeters_2015,
	title = {Neurodata {Without} {Borders}: {Creating} a {Common} {Data} {Format} for {Neurophysiology}},
	volume = {88},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627315009198},
	doi = {https://doi.org/10.1016/j.neuron.2015.10.025},
	abstract = {The Neurodata Without Borders (NWB) initiative promotes data standardization in neuroscience to increase research reproducibility and opportunities. In the first NWB pilot project, neurophysiologists and software developers produced a common data format for recordings and metadata of cellular electrophysiology and optical imaging experiments. The format specification, application programming interfaces, and sample datasets have been released.},
	number = {4},
	journal = {Neuron},
	author = {Teeters, Jeffery L. and Godfrey, Keith and Young, Rob and Dang, Chinh and Friedsam, Claudia and Wark, Barry and Asari, Hiroki and Peron, Simon and Li, Nuo and Peyrache, Adrien and Denisov, Gennady and Siegle, Joshua H. and Olsen, Shawn R. and Martin, Christopher and Chun, Miyoung and Tripathy, Shreejoy and Blanche, Timothy J. and Harris, Kenneth and Buzs{\'a}ki, Gy{\"o}rgy and Koch, Christof and Meister, Markus and Svoboda, Karel and Sommer, Friedrich T.},
	year = {2015},
	pages = {629 -- 634}
}

@article{Assante_2016,
	title = {Are {Scientific} {Data} {Repositories} {Coping} with {Research} {Data} {Publishing}?},
	volume = {15},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\textcopyright}, {\textregistered} or {\texttrademark} of their respective owners. No challenge to any owner{\textquoteright}s rights is intended or should be inferred.},
	issn = {1683-1470},
	url = {http://datascience.codata.org/articles/10.5334/dsj-2016-006/},
	doi = {10.5334/dsj-2016-006},
	abstract = {Research data publishing is intended as the release of research data to make it possible for practitioners to (re)use them according to {\textquotedblleft}open science{\textquotedblright} dynamics. There are three main actors called to deal with research data publishing practices: researchers, publishers, and data repositories. This study analyses the solutions offered by generalist scientific data repositories, i.e., repositories supporting the deposition of any type of research data. These repositories cannot make any assumption on the application domain. They are actually called to face with the almost open ended typologies of data used in science. The current practices promoted by such repositories are analysed with respect to eight key aspects of data publishing, i.e., dataset formatting, documentation, licensing, publication costs, validation, availability, discovery and access, and citation. From this analysis it emerges that these repositories implement well consolidated practices and pragmatic solutions for literature repositories. These practices and solutions can not totally meet the needs of management and use of datasets resources, especially in a context where rapid technological changes continuously open new exploitation prospects.},
	language = {eng},
	number = {0},
	urldate = {2019-08-02},
	journal = {Data Science Journal},
	author = {Assante, Massimiliano and Candela, Leonardo and Castelli, Donatella and Tani, Alice},
	month = apr,
	year = {2016},
	keywords = {Data infrastructures, Data Quality, Research Data Publishing, Scientific Data Repositories},
	pages = {6},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/W7JQCF6N/Assante et al. - 2016 - Are Scientific Data Repositories Coping with Resea.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/UZZLCTUY/dsj-2016-006.html:text/html}
}

@article{Candela_2015,
	title = {Data journals: {A} survey},
	volume = {66},
	copyright = {{\textcopyright} 2015 ASIS\&T},
	issn = {2330-1643},
	shorttitle = {Data journals},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.23358},
	doi = {10.1002/asi.23358},
	abstract = {Data occupy a key role in our information society. However, although the amount of published data continues to grow and terms such as data deluge and big data today characterize numerous (research) initiatives, much work is still needed in the direction of publishing data in order to make them effectively discoverable, available, and reusable by others. Several barriers hinder data publishing, from lack of attribution and rewards, vague citation practices, and quality issues to a rather general lack of a data-sharing culture. Lately, data journals have overcome some of these barriers. In this study of more than 100 currently existing data journals, we describe the approaches they promote for data set description, availability, citation, quality, and open access. We close by identifying ways to expand and strengthen the data journals approach as a means to promote data set access and exploitation.},
	language = {en},
	number = {9},
	urldate = {2019-08-02},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Candela, Leonardo and Castelli, Donatella and Manghi, Paolo and Tani, Alice},
	year = {2015},
	keywords = {alternative publications, data, publications},
	pages = {1747--1762},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/3PY3PNDP/Candela et al. - 2015 - Data journals A survey.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/5Z2KFNTQ/asi.html:text/html}
}

@article{Vines_2014,
	title = {The {Availability} of {Research} {Data} {Declines} {Rapidly} with {Article} {Age}},
	volume = {24},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(13)01400-0},
	doi = {10.1016/j.cub.2013.11.014},
	language = {English},
	number = {1},
	urldate = {2019-08-02},
	journal = {Current Biology},
	author = {Vines, Timothy H. and Albert, Arianne Y. K. and Andrew, Rose L. and D{\'e}barre, Florence and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Moore, Jean-S{\'e}bastien and Renaut, S{\'e}bastien and Rennison, Diana J.},
	month = jan,
	year = {2014},
	pmid = {24361065},
	pages = {94--97},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/DVXPQ97M/Vines et al. - 2014 - The Availability of Research Data Declines Rapidly.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/UEYBU58Q/S0960-9822(13)01400-0.html:text/html}
}

@article{Vines_2013,
	title = {Mandated data archiving greatly improves access to research data},
	volume = {27},
	issn = {0892-6638},
	url = {https://www.fasebj.org/doi/full/10.1096/fj.12-218164},
	doi = {10.1096/fj.12-218164},
	abstract = {The data underlying scientific papers should be accessible to researchers both now and in the future, but how best can we ensure that these data are available? Here we examine the effectiveness of four approaches to data archiving: no stated archiving policy, recommending (but not requiring) archiving, and two versions of mandating data deposition at acceptance. We control for differences between data types by trying to obtain data from papers that use a single, widespread population genetic analysis, structure. At one extreme, we found that mandated data archiving policies that require the inclusion of a data availability statement in the manuscript improve the odds of finding the data online almost 1000-fold compared to having no policy. However, archiving rates at journals with less stringent policies were only very slightly higher than those with no policy at all. We also assessed the effectiveness of asking for data directly from authors and obtained over half of the requested datasets, albeit with \~{}8 d delay and some disagreement with authors. Given the long-term benefits of data accessibility to the academic community, we believe that journal-based mandatory data archiving policies and mandatory data availability statements should be more widely adopted.{\textemdash}Vines, T. H., Andrew, R. L., Bock, D. G., Franklin, M. T., Gilbert, K. J., Kane, N. C., Moore, J-S., Moyers, B. T., Renaut, S., Rennison, D. J., Veen, T., Yeaman, S. Mandated data archiving greatly improves access to research data.},
	number = {4},
	urldate = {2019-08-02},
	journal = {The FASEB Journal},
	author = {Vines, Timothy H. and Andrew, Rose L. and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Kane, Nolan C. and Moore, Jean-S{\'e}bastien and Moyers, Brook T. and Renaut, S{\'e}bastien and Rennison, Diana J. and Veen, Thor and Yeaman, Sam},
	month = jan,
	year = {2013},
	pages = {1304--1308},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/Q9EG3ENE/Vines et al. - 2013 - Mandated data archiving greatly improves access to.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/UH85KM4F/fj.html:text/html}
}

@article{Savage_2009,
	title = {Empirical {Study} of {Data} {Sharing} by {Authors} {Publishing} in {PLoS} {Journals}},
	volume = {4},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0007078},
	doi = {10.1371/journal.pone.0007078},
	abstract = {Background Many journals now require authors share their data with other investigators, either by depositing the data in a public repository or making it freely available upon request. These policies are explicit, but remain largely untested. We sought to determine how well authors comply with such policies by requesting data from authors who had published in one of two journals with clear data sharing policies. Methods and Findings We requested data from ten investigators who had published in either PLoS Medicine or PLoS Clinical Trials. All responses were carefully documented. In the event that we were refused data, we reminded authors of the journal's data sharing guidelines. If we did not receive a response to our initial request, a second request was made. Following the ten requests for raw data, three investigators did not respond, four authors responded and refused to share their data, two email addresses were no longer valid, and one author requested further details. A reminder of PLoS's explicit requirement that authors share data did not change the reply from the four authors who initially refused. Only one author sent an original data set. Conclusions We received only one of ten raw data sets requested. This suggests that journal policies requiring data sharing do not lead to authors making their data sets available to independent investigators.},
	language = {en},
	number = {9},
	urldate = {2019-08-02},
	journal = {PLOS ONE},
	author = {Savage, Caroline J. and Vickers, Andrew J.},
	month = sep,
	year = {2009},
	keywords = {Clinical trials, Data acquisition, Genomic libraries, Genomic medicine, Health care policy, Medicine and health sciences, Open access publishing, Scientific publishing},
	pages = {e7078},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/5AGY7M74/Savage and Vickers - 2009 - Empirical Study of Data Sharing by Authors Publish.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/UT2NHCSJ/article.html:text/html}
}

@article{Baker_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	url = {http://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970},
	doi = {10.1038/533452a},
	abstract = {Survey sheds light on the {\textquoteleft}crisis{\textquoteright} rocking research.},
	language = {en},
	number = {7604},
	urldate = {2019-08-02},
	journal = {Nature News},
	author = {Baker, Monya},
	month = may,
	year = {2016},
	pages = {452},
	file = {Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/5MMVAP2S/1.html:text/html}
}

@misc{NIHGuideFINALNIHSTATEMENTONSHARINGRESEARCHDATA_,
	title = {{NIH} {Guide}: {FINAL} {NIH} {STATEMENT} {ON} {SHARING} {RESEARCH} {DATA}},
	url = {https://grants.nih.gov/grants/guide/notice-files/NOT-OD-03-032.html},
	urldate = {2019-08-02},
	file = {NIH Guide\: FINAL NIH STATEMENT ON SHARING RESEARCH DATA:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/QUN2BW6G/NOT-OD-03-032.html:text/html}
}

@article{Kwok_2018,
	title = {Lab {Notebooks} {Go} {Digital}},
	volume = {560},
	language = {en},
	journal = {Nature},
	author = {Kwok, Roberta and Kanza, S},
	year = {2018},
	pages = {269}
}

@article{Ohl_2001,
	title = {Change in pattern of ongoing cortical activity with auditory category learning},
	volume = {412},
	copyright = {2001 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35089076},
	doi = {10.1038/35089076},
	abstract = {Humans are able to classify novel items correctly by category1,2; some other animals have also been shown to do this3,4,5,6,7. During category learning, humans group perceptual stimuli by abstracting qualities from similarity relationships of their physical properties1,2,8. Forming categories is fundamental to cognition9 and can be independent of a {\^a}{\texteuro}\~{}memory store{\^a}{\texteuro}{\texttrademark} of information about the items or a prototype10. The neurophysiological mechanisms underlying the formation of categories are unknown. Using an animal model of category learning6, in which frequency-modulated tones are distinguished into the categories of {\^a}{\texteuro}\~{}rising{\^a}{\texteuro}{\texttrademark} and {\^a}{\texteuro}\~{}falling{\^a}{\texteuro}{\texttrademark} modulation, we demonstrate here that the sorting of stimuli into these categories emerges as a sudden change in an animal's learning strategy. Electro-corticographical recording from the auditory cortex11 shows that the transition is accompanied by a change in the dynamics of cortical stimulus representation. We suggest that this dynamic change represents a mechanism underlying the recognition of the abstract quality (or qualities) that defines the categories.},
	language = {en},
	number = {6848},
	urldate = {2019-03-15},
	journal = {Nature},
	author = {Ohl, F. W. and Scheich, H. and Freeman, W. J.},
	month = aug,
	year = {2001},
	pages = {733--736},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/V8QIHMJK/Ohl et al. - 2001 - Change in pattern of ongoing cortical activity wit.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/2BEWC688/35089076.html:text/html}
}

@article{Kanza_2017,
	title = {Electronic {Lab} {Notebooks}: {Can} {They} {Replace} {Paper}?},
	volume = {9},
	issn = {1758-2946},
	shorttitle = {Electronic {Lab} {Notebooks}},
	url = {http://jcheminf.springeropen.com/articles/10.1186/s13321-017-0221-3},
	doi = {10.1186/s13321-017-0221-3},
	abstract = {Despite the increasingly digital nature of society there are some areas of research that remain firmly rooted in the past; in this case the laboratory notebook, the last remaining paper component of an experiment. Countless electronic laboratory notebooks (ELNs) have been created in an attempt to digitise record keeping processes in the lab, but none of them have become a `key player' in the ELN market, due to the many adoption barriers that have been identified in previous research and further explored in the user studies presented here. The main issues identified are the cost of the current available ELNs, their ease of use (or lack of it) and their accessibility issues across different devices and operating systems. Evidence suggests that whilst scientists willingly make use of generic notebooking software, spreadsheets and other general office and scientific tools to aid their work, current ELNs are lacking in the required functionality to meet the needs of the researchers. In this paper we present our extensive research and user study results to propose an ELN built upon a pre-existing cloud notebook platform that makes use of accessible popular scientific software and semantic web technologies to help overcome the identified barriers to adoption.},
	language = {en},
	number = {1},
	urldate = {2018-12-18},
	journal = {Journal of Cheminformatics},
	author = {Kanza, Samantha and Willoughby, Cerys and Gibbins, Nicholas and Whitby, Richard and Frey, Jeremy Graham and Erjavec, Jana and Zupan{\v c}i{\v c}, Klemen and Hren, Matja{\v z} and Kova{\v c}, Katarina},
	month = dec,
	year = {2017}
}

@article{Rubacha_2011,
	title = {A {Review} of {Electronic} {Laboratory} {Notebooks} {Available} in the {Market} {Today}},
	volume = {16},
	issn = {22110682},
	url = {http://journals.sagepub.com/doi/10.1016/j.jala.2009.01.002},
	doi = {10.1016/j.jala.2009.01.002},
	language = {en},
	number = {1},
	urldate = {2018-12-18},
	journal = {Journal of Laboratory Automation},
	author = {Rubacha, Michael and Rattan, Anil K. and Hosselet, Stephen C.},
	month = feb,
	year = {2011},
	pages = {90--98}
}

@article{Deisseroth_2013,
	title = {Engineering {Approaches} to {Illuminating} {Brain} {Structure} and {Dynamics}.},
	volume = {80},
	doi = {10.1016/j.neuron.2013.10.032},
	number = {3},
	journal = {Neuron},
	author = {Deisseroth, Karl and Schnitzer, Mark J.},
	year = {2013},
	pages = {568--577}
}

@article{Geisler_2008,
	title = {Visual {Perception} and the {Statistical} {Properties} of {Natural} {Scenes}},
	volume = {59},
	issn = {1545-2085},
	doi = {10.1146/annurev.psych.58.110405.085632},
	number = {1},
	journal = {Annu. Rev. Psychol.},
	author = {Geisler, Wilson S.},
	year = {2008},
	pages = {167--192}
}

@article{Grewe_2011,
	title = {A {Bottom}-up {Approach} to {Data} {Annotation} in {Neurophysiology}},
	volume = {5},
	url = {http://dx.doi.org/10.3389/fninf.2011.00016},
	doi = {10.3389/fninf.2011.00016},
	journal = {Front. Neuroinform.},
	author = {Grewe, Jan and Wachtler, Thomas and Benda, Jan},
	year = {2011}
}

@article{Maldonado_2008,
	title = {Synchronization of {Neuronal} {Responses} in {Primary} {Visual} {Cortex} of {Monkeys} {Viewing} {Natural} {Images}},
	volume = {100},
	issn = {1522-1598},
	doi = {10.1152/jn.00076.2008},
	number = {3},
	journal = {Journal of Neurophysiology},
	author = {Maldonado, P. and Babul, C. and Singer, W. and Rodriguez, E. and Berger, D. and Gr{\"u}n, S.},
	year = {2008},
	pages = {1523--1532}
}

@article{Miyamoto_2015,
	title = {The {Fiber}-{Optic} {Imaging} and {Manipulation} of {Neural} {Activity} during {Animal} {Behavior}},
	issn = {0168-0102},
	doi = {10.1016/j.neures.2015.09.004},
	journal = {Neuroscience Research},
	author = {Miyamoto, Daisuke and Murayama, Masanori},
	year = {2015}
}

@article{Moucek_2014,
	title = {Software and hardware infrastructure for research in electrophysiology},
	volume = {8},
	journal = {Frontiers in neuroinformatics},
	author = {Mou{\v c}ek, Roman and Br{\u u}ha, Petr and Jezek, Petr and Mautner, Pavel and Novotny, Jiri and Papez, Vaclav and Prokop, Tom{\'a}{\v s} and {\v R}ond{\'i}k, Tom{\'a}{\v s} and {\v S}t{\v e}bet{\'a}k, Jan and Vareka, Lukas},
	year = {2014},
	pages = {20}
}

@article{Nicolelis_2002,
	title = {Multielectrode {Recordings}: {The} next {Steps}.},
	volume = {12},
	doi = {10.1016/S0959-4388(02)00374-4},
	number = {5},
	journal = {Curr Opin Neurobiol},
	author = {Nicolelis, Miguel A L. and Ribeiro, Sidarta},
	year = {2002},
	pages = {602--606}
}

@article{Obien_2014,
	title = {Revealing {Neuronal} {Function} through {Microelectrode} {Array} {Recordings}.},
	volume = {8},
	doi = {10.3389/fnins.2014.00423},
	journal = {Front Neurosci},
	author = {Obien, Marie Engelene J. and Deligkaris, Kosmas and Bullmann, Torsten and Bakkum, Douglas J. and Frey, Urs},
	year = {2014},
	pages = {423}
}

@article{Schwarz_2014,
	title = {Chronic, {Wireless} {Recordings} of {Large}-{Scale} {Brain} {Activity} in {Freely} {Moving} {Rhesus} {Monkeys}},
	volume = {11},
	issn = {1548-7105},
	doi = {10.1038/nmeth.2936},
	number = {6},
	journal = {Nat Meth},
	author = {Schwarz, David A and Lebedev, Mikhail A and Hanson, Timothy L and Dimitrov, Dragan F and Lehew, Gary and Meloy, Jim and Rajangam, Sankaranarayani and Subramanian, Vivek and Ifft, Peter J and Li, Zheng and al, et},
	year = {2014},
	pages = {670--676}
}

@article{Willoughby_2014,
	title = {Creating {Context} for the {Experiment} {Record}. {User}-{Defined} {Metadata}: {Investigations} into {Metadata} {Usage} in the {LabTrove} {ELN}},
	volume = {54},
	issn = {1549-9596},
	url = {https://doi.org/10.1021/ci500469f},
	doi = {10.1021/ci500469f},
	number = {12},
	journal = {Journal of Chemical Information and Modeling},
	author = {Willoughby, Cerys and Bird, Colin L. and Coles, Simon J. and Frey, Jeremy G.},
	month = dec,
	year = {2014},
	pages = {3268--3283}
}

@article{Bzdok_2017,
	title = {Inference in the age of big data: {Future} perspectives on neuroscience},
	volume = {155},
	issn = {1053-8119},
	shorttitle = {Inference in the age of big data},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811917303816},
	doi = {10.1016/j.neuroimage.2017.04.061},
	abstract = {Neuroscience is undergoing faster changes than ever before. Over 100 years our field qualitatively described and invasively manipulated single or few organisms to gain anatomical, physiological, and pharmacological insights. In the last 10 years neuroscience spawned quantitative datasets of unprecedented breadth (e.g., microanatomy, synaptic connections, and optogenetic brain-behavior assays) and size (e.g., cognition, brain imaging, and genetics). While growing data availability and information granularity have been amply discussed, we direct attention to a less explored question: How will the unprecedented data richness shape data analysis practices? Statistical reasoning is becoming more important to distill neurobiological knowledge from healthy and pathological brain measurements. We argue that large-scale data analysis will use more statistical models that are non-parametric, generative, and mixing frequentist and Bayesian aspects, while supplementing classical hypothesis testing with out-of-sample predictions.},
	urldate = {2018-11-14},
	journal = {NeuroImage},
	author = {Bzdok, Danilo and Yeo, B. T. Thomas},
	month = jul,
	year = {2017},
	keywords = {Epistemology, High-dimensional statistics, Hypothesis testing, Machine learning, Sample complexity, Systems biology},
	pages = {549--564},
	file = {ScienceDirect Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/QG59ID5F/Bzdok and Yeo - 2017 - Inference in the age of big data Future perspecti.pdf:application/pdf;ScienceDirect Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/MDWBX46V/S1053811917303816.html:text/html}
}

@inproceedings{Coles_2008,
	title = {Experiences with repositories and blogs in laboratories},
	url = {https://eprints.soton.ac.uk/50901/},
	booktitle = {Open {Repositories} 2008},
	author = {Coles, Simon and Carr, Leslie and Frey, Jeremy},
	month = apr,
	year = {2008},
	keywords = {cyberinfrastructure, e-science, preservation, scientific data repositories}
}

@article{Foltz_1999,
	title = {Guidelines for {Assessing} the {Health} and {Condition} of {Mice}},
	volume = {28},
	language = {en},
	number = {4},
	journal = {Lab Animal},
	author = {Foltz, Charmaine J and Ullman-Cullere, Mollie},
	year = {1999},
	pages = {5},
	file = {Foltz and Ullman-Cullere - 1999 - Guidelines for Assessing the Health and Condition .pdf:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/QDNIM9PD/Foltz and Ullman-Cullere - 1999 - Guidelines for Assessing the Health and Condition .pdf:application/pdf}
}

@article{Burkholder_2012,
	title = {Health {Evaluation} of {Experimental} {Laboratory} {Mice}},
	volume = {2},
	issn = {2161-2617},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3399545/},
	doi = {10.1002/9780470942390.mo110217},
	abstract = {Good science and good animal care go hand in hand. A sick or distressed animal does not produce the reliable results that a healthy and unstressed animal produces. This unit describes the essentials of assessing mouse health, colony health surveillance, common conditions, and determination of appropriate endpoints. Understanding the health and well-being of the mice used in research enables the investigator to optimize research results and animal care.},
	urldate = {2019-02-08},
	journal = {Curr Protoc Mouse Biol},
	author = {Burkholder, Tanya and Foltz, Charmaine and Karlsson, Eleanor and Linton, C Garry and Smith, Joanne M},
	month = jun,
	year = {2012},
	pmid = {22822473},
	pmcid = {PMC3399545},
	pages = {145--165},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/FHINY3JX/Burkholder et al. - 2012 - Health Evaluation of Experimental Laboratory Mice.pdf:application/pdf}
}

@incollection{Denker_2016,
	address = {Cham},
	title = {Designing {Workflows} for the {Reproducible} {Analysis} of {Electrophysiological} {Data}},
	volume = {10087},
	isbn = {978-3-319-50861-0 978-3-319-50862-7},
	url = {http://link.springer.com/10.1007/978-3-319-50862-7_5},
	urldate = {2016-12-14},
	booktitle = {Brain-{Inspired} {Computing}},
	publisher = {Springer International Publishing},
	author = {Denker, Michael and Gr{\~A}{\textonequarter}n, Sonja},
	editor = {Amunts, Katrin and Grandinetti, Lucio and Lippert, Thomas and Petkov, Nicolai},
	year = {2016},
	pages = {58--72}
}

@article{Vargas-Irwin_2010,
	title = {Decoding {Complete} {Reach} and {Grasp} {Actions} from {Local} {Primary} {Motor} {Cortex} {Populations}},
	volume = {30},
	issn = {1529-2401},
	doi = {10.1523/jneurosci.5443-09.2010},
	number = {29},
	journal = {Journal of Neuroscience},
	author = {Vargas-Irwin, C. E. and Shakhnarovich, G. and Yadollahpour, P. and Mislow, J. M. K. and Black, M. J. and Donoghue, J. P.},
	year = {2010},
	pages = {9659--9669}
}

@article{Verkhratsky_2006,
	title = {From {Galvani} to {Patch} {Clamp}: {The} {Development} of {Electrophysiology}.},
	volume = {453},
	doi = {10.1007/s00424-006-0169-z},
	number = {3},
	journal = {Pflugers Arch},
	author = {Verkhratsky, Alexei and Krishtal, O. A. and Petersen, Ole H.},
	year = {2006},
	pages = {233--247}
}

@article{Zehl_2016,
	title = {Handling {Metadata} in a {Neurophysiology} {Laboratory}},
	volume = {10},
	url = {http://dx.doi.org/10.3389/fninf.2016.00026},
	doi = {10.3389/fninf.2016.00026},
	journal = {Front. Neuroinform.},
	author = {Zehl, Lyuba and Jaillet, Florent and Stoewer, Adrian and Grewe, Jan and Sobolev, Andrey and Wachtler, Thomas and Brochier, Thomas G. and Riehle, Alexa and Denker, Michael and Gr{\~A}{\textonequarter}n, Sonja},
	month = jul,
	year = {2016}
}

@article{Brochier_2018,
	title = {Massively parallel recordings in macaque motor cortex during an instructed delayed reach-to-grasp task},
	volume = {5},
	url = {https://doi.org/10.1038/sdata.2018.55},
	doi = {10.1038/sdata.2018.55},
	journal = {Scientific Data},
	author = {Brochier, Thomas and Zehl, Lyuba and Hao, Yaoyao and Duret, Margaux and Sprenger, Julia and Denker, Michael and Gr{\~A}{\textonequarter}n, Sonja and Riehle, Alex},
	month = apr,
	year = {2018}
}

@article{Yatsenko_2015,
	title = {{DataJoint}: {Managing} {Big} {Scientific} {Data} {Using} {MATLAB} or {Python}},
	shorttitle = {{DataJoint}},
	url = {http://biorxiv.org/lookup/doi/10.1101/031658},
	doi = {10.1101/031658},
	abstract = {The rise of big data in modern research poses serious challenges for data management: Large and intricate datasets from diverse instrumentation must be precisely aligned, annotated, and processed in a variety of ways to extract new insights. While high levels of data integrity are expected, research teams have diverse backgrounds, are geographically dispersed, and rarely possess a primary interest in data science. Here we describe DataJoint, an open-source toolbox designed for manipulating and processing scientific data under the relational data model. Designed for scientists who need a flexible and expressive database language with few basic concepts and operations, DataJoint facilitates multi-user access, efficient queries, and distributed computing. With implementations in both MATLAB and Python, DataJoint is not limited to particular file formats, acquisition systems, or data modalities and can be quickly adapted to new experimental designs. DataJoint and related resources are available at http://datajoint.github.com.},
	urldate = {2019-01-23},
	journal = {bioRxiv},
	author = {Yatsenko, Dimitri and Reimer, Jacob and Ecker, Alexander S and Walker, Edgar Y and Sinz, Fabian and Berens, Philipp and Hoenselaar, Andreas and Cotton, Ronald James and Siapas, Athanassios S. and Tolias, Andreas S.},
	month = nov,
	year = {2015}
}

@article{Jacob_2010,
	title = {The {Matrix}: {A} {New} {Tool} for {Probing} the {Whisker}-to-{Barrel} {System} with {Natural} {Stimuli}},
	volume = {189},
	issn = {01650270},
	shorttitle = {The {Matrix}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027010001548},
	doi = {10.1016/j.jneumeth.2010.03.020},
	abstract = {The whisker to barrel system in rodents has become one of the major models for the study of sensory processing. Several tens of whiskers (or vibrissae) are distributed in a regular manner on both sides of the snout. Many tactile discrimination tasks using this system need multiple contacts with more than one whisker to be solved. With the aim of mimicking those multi-whisker stimuli during electrophysiological recordings, we developed a novel mechanical stimulator composed of 24 independent multi-directional piezoelectric benders adapted to the five rows and the five caudal arcs of the rat whisker pad. The most widely used technology for producing mechanical deflections of the whiskers is based on piezoelectric benders that display a non-linear behavior when driven with high frequency input commands and, if not compensated, show high unwanted ringing at particular resonance frequencies. If not corrected, this nonlinear behavior precludes the application of high frequency deflections and the study of cortical responses to behaviorally relevant stimuli. To cope with the ringing problem, a mechanical and a software based solutions have been developed. With these corrections, the upper bound of the linear range of the bender is increased to 1 kHz. This new device allows the controlled delivery of large scale natural patterns of whisker deflections characterized by rapid high frequency vibrations of multiple whiskers.},
	language = {en},
	number = {1},
	urldate = {2019-02-28},
	journal = {Journal of Neuroscience Methods},
	author = {Jacob, Vincent and Estebanez, Luc and Le Cam, Julie and Tiercelin, Jean-Yves and Parra, Patrick and Par{\'e}sys, G{\'e}rard and Shulz, Daniel E.},
	month = may,
	year = {2010},
	pages = {65--74}
}

@article{Denker_2018,
	title = {{LFP} {Beta} {Amplitude} {Is} {Linked} to {Mesoscopic} {Spatio}-{Temporal} {Phase} {Patterns}},
	volume = {8},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-018-22990-7},
	doi = {10.1038/s41598-018-22990-7},
	language = {en},
	number = {1},
	urldate = {2018-04-03},
	journal = {Scientific Reports},
	author = {Denker, Michael and Zehl, Lyuba and Kilavik, Bj{\o}rg E. and Diesmann, Markus and Brochier, Thomas and Riehle, Alexa and Gr{\"u}n, Sonja},
	month = dec,
	year = {2018},
	pages = {5200}
}

@article{Bitzenhofer_2017,
	title = {Layer-specific optogenetic activation of pyramidal neurons causes beta{\^a}{\texteuro}{\textquotedblleft}gamma entrainment of neonatal networks},
	volume = {8},
	copyright = {{\^A}{\textcopyright} 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
	issn = {2041-1723},
	url = {http://www.nature.com/ncomms/2017/170220/ncomms14563/full/ncomms14563.html},
	doi = {10.1038/ncomms14563},
	abstract = {Oscillations in cortical activity during development are important for functional maturation. Here, the authors use optogenetics in neonatal mice to determine a causal role for pyramidal cell firing in different prelimbic cortex layers in generating beta{\^a}{\texteuro}{\textquotedblleft}gamma range activity.},
	language = {en},
	urldate = {2017-02-24},
	journal = {Nature Communications},
	author = {Bitzenhofer, Sebastian H. and Ahlbeck, Joachim and Wolff, Amy and Wiegert, J. Simon and Gee, Christine E. and Oertner, Thomas G. and Hanganu-Opatz, Ileana L.},
	month = feb,
	year = {2017},
	pages = {14563},
	file = {Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/J98P88D4/ncomms14563.html:text/html}
}

@article{Garcia_2014,
	title = {Neo: {An} {Object} {Model} for {Handling} {Electrophysiology} {Data} in {Multiple} {Formats}},
	volume = {8},
	issn = {1662-5196},
	shorttitle = {Neo},
	url = {http://journal.frontiersin.org/article/10.3389/fninf.2014.00010/abstract},
	doi = {10.3389/fninf.2014.00010},
	urldate = {2016-03-01},
	journal = {Frontiers in Neuroinformatics},
	author = {Garcia, Samuel and Guarino, Domenico and Jaillet, Florent and Jennings, Todd and Pr{\"o}pper, Robert and Rautenberg, Philipp L. and Rodgers, Chris C. and Sobolev, Andrey and Wachtler, Thomas and Yger, Pierre and Davison, Andrew P.},
	year = {2014},
	pages = {10}
}

@article{Stoewer_2014,
	title = {File {Format} and {Library} for {Neuroscience} {Data} and {Metadata}},
	issn = {1662-5196},
	url = {http://www.frontiersin.org/Community/AbstractDetails.aspx?ABS_DOI=10.3389/conf.fninf.2014.18.00027},
	doi = {10.3389/conf.fninf.2014.18.00027},
	urldate = {2019-01-23},
	journal = {Frontiers in Neuroinformatics Conference Abstract: Neuroinformatics 2014},
	author = {Stoewer, Adrian and Kellner, Christian and Benda, Jan and Wachtler, Thomas and Grewe, Jan},
	year = {2014}
}

@article{Wilkinson_2016,
	title = {The {FAIR} {Guiding} {Principles} for {Scientific} {Data} {Management} and {Stewardship}},
	volume = {3},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	urldate = {2018-03-14},
	journal = {Scientific Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Merc{\`e} and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and 't Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	month = mar,
	year = {2016},
	pages = {160018}
}

@article{Tebaykin_2017,
	title = {Modeling sources of interlaboratory variability in electrophysiological properties of mammalian neurons},
	volume = {119},
	url = {https://doi.org/10.1152/jn.00604.2017},
	doi = {10.1152/jn.00604.2017},
	abstract = {Patch-clamp electrophysiology is widely used to characterize neuronal electrical phenotypes. However, there are no standard experimental conditions for in vitro whole cell patch-clamp electrophysiology, complicating direct comparisons between data sets. In this study, we sought to understand how basic experimental conditions differ among laboratories and how these differences might impact measurements of electrophysiological parameters. We curated the compositions of external bath solutions (artificial cerebrospinal fluid), internal pipette solutions, and other methodological details such as animal strain and age from 509 published neurophysiology articles studying rodent neurons. We found that very few articles used the exact same experimental solutions as any other, and some solution differences stem from recipe inheritance from advisor to advisee as well as changing trends over the years. Next, we used statistical models to understand how the use of different experimental conditions impacts downstream electrophysiological measurements such as resting potential and action potential width. Although these experimental condition features could explain up to 43\% of the study-to-study variance in electrophysiological parameters, the majority of the variability was left unexplained. Our results suggest that there are likely additional experimental factors that contribute to cross-laboratory electrophysiological variability, and identifying and addressing these will be important to future efforts to assemble consensus descriptions of neurophysiological phenotypes for mammalian cell types.NEW \& NOTEWORTHY This article describes how using different experimental methods during patch-clamp electrophysiology impacts downstream physiological measurements. We characterized how methodologies and experimental solutions differ across articles. We found that differences in methods can explain some, but not all, of the study-to-study variance in electrophysiological measurements. Explicitly accounting for methodological differences using statistical models can help correct downstream electrophysiological measurements for cross-laboratory methodology differences.},
	number = {4},
	journal = {Journal of Neurophysiology},
	author = {Tebaykin, Dmitry and Tripathy, Shreejoy J. and Binnion, Nathalie and Li, Brenna and Gerkin, Richard C. and Pavlidis, Paul},
	year = {2017},
	pmid = {29357465},
	pages = {1329--1339}
}

@article{Chen_2019,
	title = {Open is not enough},
	volume = {15},
	copyright = {2018 Springer Nature Limited},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-018-0342-2},
	doi = {10.1038/s41567-018-0342-2},
	abstract = {The solutions adopted by the high-energy physics community to foster reproducible research are examples of best practices that could be embraced more widely. This first experience suggests that reproducibility requires going beyond openness.},
	language = {En},
	number = {2},
	urldate = {2019-08-02},
	journal = {Nature Physics},
	author = {Chen, Xiaoli and Dallmeier-Tiessen, S{\"u}nje and Dasler, Robin and Feger, Sebastian and Fokianos, Pamfilos and Gonzalez, Jose Benito and Hirvonsalo, Harri and Kousidis, Dinos and Lavasa, Artemis and Mele, Salvatore and Rodriguez, Diego Rodriguez and {\v S}imko, Tibor and Smith, Tim and Trisovic, Ana and Trzcinska, Anna and Tsanaktsidis, Ioannis and Zimmermann, Markus and Cranmer, Kyle and Heinrich, Lukas and Watts, Gordon and Hildreth, Michael and Iglesias, Lara Lloret and Lassila-Perini, Kati and Neubert, Sebastian},
	month = feb,
	year = {2019},
	pages = {113},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/2KZC32SR/Chen et al. - 2019 - Open is not enough.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/BP67FLHS/s41567-018-0342-2.html:text/html}
}

@misc{PEP8StyleGuideforPythonCode_,
	title = {{PEP} 8 -- {Style} {Guide} for {Python} {Code}},
	url = {https://www.python.org/dev/peps/pep-0008/},
	abstract = {The official home of the Python Programming Language},
	language = {en},
	urldate = {2019-08-02},
	journal = {Python.org},
	file = {Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/73GWKZXL/pep-0008.html:text/html}
}

@article{Eisner_2018,
	title = {Reproducibility of science: {Fraud}, impact factors and carelessness},
	volume = {114},
	issn = {0022-2828},
	shorttitle = {Reproducibility of science},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6565841/},
	doi = {10.1016/j.yjmcc.2017.10.009},
	abstract = {There is great concern that results published in a large fraction of biomedical papers may not be reproducible. This article reviews the evidence for this and considers some of the factors that are responsible and how the problem may be solved. One issue is scientific fraud. This, in turn, may result from pressures put on scientists to succeed including the need to publish in {\textquotedblleft}high impact{\textquotedblright} journals. I emphasise the importance of judging the quality of the science itself as opposed to using surrogate metrics. The other factors discussed include problems of experimental design and statistical analysis of the work. It is important that these issues are addressed by the scientific community before others impose draconian regulations., 
          
            
              {\textbullet}
              Much of science cannot be reproduced; this article reviews the causes.
            
            
              {\textbullet}
              Fraud, sometimes encouraged by the need to publish in high impact journals contributes.
            
            
              {\textbullet}
              Another, major factor, is poor experimental design and statistical analysis.
            
            
              {\textbullet}
              The scientific community needs to reform before change is thrust upon it.},
	urldate = {2019-08-03},
	journal = {J Mol Cell Cardiol},
	author = {Eisner, D.A.},
	month = jan,
	year = {2018},
	pmid = {29079076},
	pmcid = {PMC6565841},
	pages = {364--368},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/A8I7F3BC/Eisner - 2018 - Reproducibility of science Fraud, impact factors .pdf:application/pdf}
}

@article{Fidler_2017,
	title = {Metaresearch for {Evaluating} {Reproducibility} in {Ecology} and {Evolution}},
	volume = {67},
	issn = {0006-3568},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5384162/},
	doi = {10.1093/biosci/biw159},
	abstract = {Recent replication projects in other disciplines have uncovered disturbingly low levels of reproducibility, suggesting that those research literatures may contain unverifiable claims. The conditions contributing to irreproducibility in other disciplines are also present in ecology. These include a large discrepancy between the proportion of {\textquotedblleft}positive{\textquotedblright} or {\textquotedblleft}significant{\textquotedblright} results and the average statistical power of empirical research, incomplete reporting of sampling stopping rules and results, journal policies that discourage replication studies, and a prevailing publish-or-perish research culture that encourages questionable research practices. We argue that these conditions constitute sufficient reason to systematically evaluate the reproducibility of the evidence base in ecology and evolution. In some cases, the direct replication of ecological research is difficult because of strong temporal and spatial dependencies, so here, we propose metaresearch projects that will provide proxy measures of reproducibility.},
	number = {3},
	urldate = {2019-08-03},
	journal = {Bioscience},
	author = {Fidler, Fiona and Chee, Yung En and Wintle, Bonnie C. and Burgman, Mark A. and McCarthy, Michael A. and Gordon, Ascelin},
	month = mar,
	year = {2017},
	pmid = {28596617},
	pmcid = {PMC5384162},
	pages = {282--289},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/2767MGVL/Fidler et al. - 2017 - Metaresearch for Evaluating Reproducibility in Eco.pdf:application/pdf}
}

@article{Steinfath_2018,
	title = {Simple changes of individual studies can improve the reproducibility of the biomedical scientific process as a whole},
	volume = {13},
	issn = {1932-6203},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6135363/},
	doi = {10.1371/journal.pone.0202762},
	abstract = {We developed a new probabilistic model to assess the impact of recommendations rectifying the reproducibility crisis (by publishing both positive and {\textquoteleft}negative{\textquoteleft} results and increasing statistical power) on competing objectives, such as discovering causal relationships, avoiding publishing false positive results, and reducing resource consumption. In contrast to recent publications our model quantifies the impact of each single suggestion not only for an individual study but especially their relation and consequences for the overall scientific process. We can prove that higher-powered experiments can save resources in the overall research process without generating excess false positives. The better the quality of the pre-study information and its exploitation, the more likely this beneficial effect is to occur. Additionally, we quantify the adverse effects of both neglecting good practices in the design and conduct of hypotheses-based research, and the omission of the publication of {\textquoteleft}negative{\textquoteleft} findings. Our contribution is a plea for adherence to or reinforcement of the good scientific practice and publication of {\textquoteleft}negative{\textquoteleft} findings.},
	number = {9},
	urldate = {2019-08-03},
	journal = {PLoS One},
	author = {Steinfath, Matthias and Vogl, Silvia and Violet, Norman and Schwarz, Franziska and Mielke, Hans and Selhorst, Thomas and Greiner, Matthias and Sch{\"o}nfelder, Gilbert},
	month = sep,
	year = {2018},
	pmid = {30208060},
	pmcid = {PMC6135363},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/BHWYTSYI/Steinfath et al. - 2018 - Simple changes of individual studies can improve t.pdf:application/pdf}
}

@article{VanBavel_2016,
	title = {Contextual sensitivity in scientific reproducibility},
	volume = {113},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4988618/},
	doi = {10.1073/pnas.1521897113},
	abstract = {Scientific progress requires that findings can be reproduced by other scientists. However, there is widespread debate in psychology (and other fields) about how to interpret failed replications. Many have argued that contextual factors might account for several of these failed replications. We analyzed 100 replication attempts in psychology and found that the extent to which the research topic was likely to be contextually sensitive (varying in time, culture, or location) was associated with replication success. This relationship remained a significant predictor of replication success even after adjusting for characteristics of the original and replication studies that previously had been associated with replication success (e.g., effect size, statistical power). We offer recommendations for psychologists and other scientists interested in reproducibility., In recent years, scientists have paid increasing attention to reproducibility. For example, the Reproducibility Project, a large-scale replication attempt of 100 studies published in top psychology journals found that only 39\% could be unambiguously reproduced. There is a growing consensus among scientists that the lack of reproducibility in psychology and other fields stems from various methodological factors, including low statistical power, researcher{\textquoteright}s degrees of freedom, and an emphasis on publishing surprising positive results. However, there is a contentious debate about the extent to which failures to reproduce certain results might also reflect contextual differences (often termed {\textquotedblleft}hidden moderators{\textquotedblright}) between the original research and the replication attempt. Although psychologists have found extensive evidence that contextual factors alter behavior, some have argued that context is unlikely to influence the results of direct replications precisely because these studies use the same methods as those used in the original research. To help resolve this debate, we recoded the 100 original studies from the Reproducibility Project on the extent to which the research topic of each study was contextually sensitive. Results suggested that the contextual sensitivity of the research topic was associated with replication success, even after statistically adjusting for several methodological characteristics (e.g., statistical power, effect size). The association between contextual sensitivity and replication success did not differ across psychological subdisciplines. These results suggest that researchers, replicators, and consumers should be mindful of contextual factors that might influence a psychological process. We offer several guidelines for dealing with contextual sensitivity in reproducibility.},
	number = {23},
	urldate = {2019-08-03},
	journal = {Proc Natl Acad Sci U S A},
	author = {Van Bavel, Jay J. and Mende-Siedlecki, Peter and Brady, William J. and Reinero, Diego A.},
	month = jun,
	year = {2016},
	pmid = {27217556},
	pmcid = {PMC4988618},
	pages = {6454--6459},
	file = {PubMed Central Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/SYYA86IT/Van Bavel et al. - 2016 - Contextual sensitivity in scientific reproducibili.pdf:application/pdf}
}

@article{Ioannidis_2005,
	title = {Why most published research findings are false},
	volume = {2},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	language = {eng},
	number = {8},
	journal = {PLoS Med.},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pmid = {16060722},
	pmcid = {PMC1182327},
	keywords = {Bias, Data Interpretation, Statistical, Likelihood Functions, Meta-Analysis as Topic, Odds Ratio, Publishing, Reproducibility of Results, Research Design, Sample Size},
	pages = {e124},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/DQP9VKJH/Ioannidis - 2005 - Why most published research findings are false.pdf:application/pdf}
}

@article{Goodman_2007,
	title = {Why most published research findings are false: problems in the analysis},
	volume = {4},
	issn = {1549-1676},
	shorttitle = {Why most published research findings are false},
	doi = {10.1371/journal.pmed.0040168},
	language = {eng},
	number = {4},
	journal = {PLoS Med.},
	author = {Goodman, Steven and Greenland, Sander},
	month = apr,
	year = {2007},
	pmid = {17456002},
	pmcid = {PMC1855693},
	keywords = {Deception, Humans, Periodicals as Topic, Publishing, Research Design, Statistics as Topic},
	pages = {e168},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/2HJQG63P/Goodman and Greenland - 2007 - Why most published research findings are false pr.pdf:application/pdf}
}

@article{Ioannidis_2007,
	title = {Why most published research findings are false: author's reply to {Goodman} and {Greenland}},
	volume = {4},
	issn = {1549-1676},
	shorttitle = {Why most published research findings are false},
	doi = {10.1371/journal.pmed.0040215},
	language = {eng},
	number = {6},
	journal = {PLoS Med.},
	author = {Ioannidis, John P. A.},
	month = jun,
	year = {2007},
	pmid = {17593900},
	pmcid = {PMC1896210},
	keywords = {Humans, Periodicals as Topic, Publishing, Reproducibility of Results, Research Design},
	pages = {e215},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/ZBENZQPI/Ioannidis - 2007 - Why most published research findings are false au.pdf:application/pdf}
}

@article{Pashler_2012,
	title = {Editors{\textquoteright} {Introduction} to the {Special} {Section} on {Replicability} in {Psychological} {Science}: {A} {Crisis} of {Confidence}?},
	volume = {7},
	issn = {1745-6916},
	shorttitle = {Editors{\textquoteright} {Introduction} to the {Special} {Section} on {Replicability} in {Psychological} {Science}},
	url = {https://doi.org/10.1177/1745691612465253},
	doi = {10.1177/1745691612465253},
	language = {en},
	number = {6},
	urldate = {2019-08-03},
	journal = {Perspect Psychol Sci},
	author = {Pashler, Harold and Wagenmakers, Eric{\textendash}Jan},
	month = nov,
	year = {2012},
	pages = {528--530},
	file = {SAGE PDF Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/M6VJ5JQS/Pashler and Wagenmakers - 2012 - Editors{\textquoteright} Introduction to the Special Section on Re.pdf:application/pdf}
}

@inproceedings{Drummond_2009,
	title = {Replicability is not reproducibility: {Nor} is it good science},
	shorttitle = {Replicability is not reproducibility},
	abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this prob-lem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of ex-perimental results is the hallmark of science. Therefore, given that most of us regard ma-chine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally de-sirable property, from replicability, its poor cousin. I claim there are important differ-ences between the two. Reproducibility re-quires changes; replicability avoids them. Al-though reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having. 1.},
	booktitle = {In {Proceedings} of the {Evaluation} {Methods} for {Machine} {Learning} {Workshop} at the 26th {ICML}},
	author = {Drummond, Chris},
	year = {2009},
	file = {Citeseer - Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/VGYM7YT5/Drummond - 2009 - Replicability is not reproducibility Nor is it go.pdf:application/pdf;Citeseer - Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/EKEC8IS5/summary.html:text/html}
}

@article{OpenScienceCollaboration_2015,
	title = {{PSYCHOLOGY}. {Estimating} the reproducibility of psychological science},
	volume = {349},
	issn = {1095-9203},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	language = {eng},
	number = {6251},
	journal = {Science},
	author = {{Open Science Collaboration}},
	month = aug,
	year = {2015},
	pmid = {26315443},
	keywords = {Behavioral Research, Confidence Intervals, Periodicals as Topic, Psychology, Publication Bias, Publishing, Reproducibility of Results, Research, Research Design},
	pages = {aac4716},
	file = {Accepted Version:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/EQQ5EKVG/Open Science Collaboration - 2015 - PSYCHOLOGY. Estimating the reproducibility of psyc.pdf:application/pdf}
}

@article{Anderson_2016,
	title = {Response to {Comment} on "{Estimating} the reproducibility of psychological science"},
	volume = {351},
	issn = {1095-9203},
	doi = {10.1126/science.aad9163},
	abstract = {Gilbert et al. conclude that evidence from the Open Science Collaboration's Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.},
	language = {eng},
	number = {6277},
	journal = {Science},
	author = {Anderson, Christopher J. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Barnett-Cowan, Michael and Bosco, Frank A. and Chandler, Jesse and Chartier, Christopher R. and Cheung, Felix and Christopherson, Cody D. and Cordes, Andreas and Cremata, Edward J. and Della Penna, Nicolas and Estel, Vivien and Fedor, Anna and Fitneva, Stanka A. and Frank, Michael C. and Grange, James A. and Hartshorne, Joshua K. and Hasselman, Fred and Henninger, Felix and van der Hulst, Marije and Jonas, Kai J. and Lai, Calvin K. and Levitan, Carmel A. and Miller, Jeremy K. and Moore, Katherine S. and Meixner, Johannes M. and Munaf{\`o}, Marcus R. and Neijenhuijs, Koen I. and Nilsonne, Gustav and Nosek, Brian A. and Plessow, Franziska and Prenoveau, Jason M. and Ricker, Ashley A. and Schmidt, Kathleen and Spies, Jeffrey R. and Stieger, Stefan and Strohminger, Nina and Sullivan, Gavin B. and van Aert, Robbie C. M. and van Assen, Marcel A. L. M. and Vanpaemel, Wolf and Vianello, Michelangelo and Voracek, Martin and Zuni, Kellylynn},
	month = mar,
	year = {2016},
	pmid = {26941312},
	keywords = {Behavioral Research, Psychology, Publishing, Research},
	pages = {1037},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/PT8C5FWR/Anderson et al. - 2016 - Response to Comment on Estimating the reproducibi.pdf:application/pdf}
}

@article{Vines_2014a,
	title = {The {Availability} of {Research} {Data} {Declines} {Rapidly} with {Article} {Age}},
	volume = {24},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(13)01400-0},
	doi = {10.1016/j.cub.2013.11.014},
	language = {English},
	number = {1},
	urldate = {2019-08-03},
	journal = {Current Biology},
	author = {Vines, Timothy H. and Albert, Arianne Y. K. and Andrew, Rose L. and D{\'e}barre, Florence and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Moore, Jean-S{\'e}bastien and Renaut, S{\'e}bastien and Rennison, Diana J.},
	month = jan,
	year = {2014},
	pmid = {24361065},
	pages = {94--97},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/UYVCR3LQ/Vines et al. - 2014 - The Availability of Research Data Declines Rapidly.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/ERRI54CM/S0960-9822(13)01400-0.html:text/html}
}

@article{Savage_2009a,
	title = {Empirical study of data sharing by authors publishing in {PLoS} journals},
	volume = {4},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0007078},
	abstract = {BACKGROUND: Many journals now require authors share their data with other investigators, either by depositing the data in a public repository or making it freely available upon request. These policies are explicit, but remain largely untested. We sought to determine how well authors comply with such policies by requesting data from authors who had published in one of two journals with clear data sharing policies.
METHODS AND FINDINGS: We requested data from ten investigators who had published in either PLoS Medicine or PLoS Clinical Trials. All responses were carefully documented. In the event that we were refused data, we reminded authors of the journal's data sharing guidelines. If we did not receive a response to our initial request, a second request was made. Following the ten requests for raw data, three investigators did not respond, four authors responded and refused to share their data, two email addresses were no longer valid, and one author requested further details. A reminder of PLoS's explicit requirement that authors share data did not change the reply from the four authors who initially refused. Only one author sent an original data set.
CONCLUSIONS: We received only one of ten raw data sets requested. This suggests that journal policies requiring data sharing do not lead to authors making their data sets available to independent investigators.},
	language = {eng},
	number = {9},
	journal = {PLoS ONE},
	author = {Savage, Caroline J. and Vickers, Andrew J.},
	month = sep,
	year = {2009},
	pmid = {19763261},
	pmcid = {PMC2739314},
	keywords = {Access to Information, Biomedical Research, Conflict of Interest, Cooperative Behavior, Editorial Policies, Humans, Periodicals as Topic, Publishing, Research Personnel},
	pages = {e7078},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/TLNQ26LP/Savage and Vickers - 2009 - Empirical study of data sharing by authors publish.pdf:application/pdf}
}

@article{Unakafova_2019,
	title = {Comparing open-source toolboxes for processing and analysis of spike and local field potentials data},
	copyright = {{\textcopyright} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/600486v2},
	doi = {10.1101/600486},
	abstract = {{\textless}h3{\textgreater}ABSTRACT{\textless}/h3{\textgreater} {\textless}p{\textgreater}Analysis of spike and local field potential (LFP) data is an essential part of neuroscientific research. Today there exist many open-source toolboxes for spike and LFP data analysis implementing various functionality. Here we aim to provide a practical guidance for neuroscientists in the choice of an open-source toolbox best satisfying their needs. We overview major open-source toolboxes for spike and LFP data analysis as well as toolboxes with tools for connectivity analysis, dimensionality reduction and generalized linear modeling. We focus on comparing toolboxes functionality, statistical and visualization tools, documentation and support quality. To give a better insight, we compare and illustrate functionality of the toolboxes on open-access dataset or simulated data and make corresponding MATLAB scripts publicly available.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-08-03},
	journal = {bioRxiv},
	author = {Unakafova, Valentina A. and Gail, Alexander},
	month = apr,
	year = {2019},
	pages = {600486},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/GY4IFUTA/Unakafova and Gail - 2019 - Comparing open-source toolboxes for processing and.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/7NG2R3AZ/600486v2.html:text/html}
}

@article{Yeung_2017,
	title = {Do {Neuroscience} {Journals} {Accept} {Replications}? {A} {Survey} of {Literature}},
	volume = {11},
	issn = {1662-5161},
	shorttitle = {Do {Neuroscience} {Journals} {Accept} {Replications}?},
	url = {https://www.frontiersin.org/articles/10.3389/fnhum.2017.00468/full},
	doi = {10.3389/fnhum.2017.00468},
	abstract = {Background: Recent reports in neuroscience, especially those concerning brain-injury and neuroimaging, have revealed low reproducibility of results within the field and urged for more replication studies. However, it is unclear if the neuroscience journals welcome or discourage the submission of reports on replication studies. Therefore, the current study assessed the explicit position of neuroscience journals on replications. Methods: A list of active neuroscience journals publishing in English was compiled from Scopus database. These journal websites were accessed to read their aims and scope and instructions to authors, and to assess if they (1) explicitly stated that they accept replications; (2) did not state their position on replications; (3) implicitly discouraged replications by emphasizing on the novelty of the manuscripts; or (4) explicitly stated that they reject replications. For journals that explicitly stated they accept or reject replications, their subcategory within neuroscience and their 5-year impact factor were recorded. The distribution of neuroscience replication studies published was also recorded by searching and extracting data from Scopus. Results: Of the 465 journals reviewed, 28 (6.0\%) explicitly stated that they accept replications, 394 (84.7\%) did not state their position on replications, 40 (8.6\%) implicitly discouraged replications by emphasizing on the novelty of the manuscripts, and 3 (0.6\%) explicitly stated that they reject replications. For the 28 journals that explicitly welcomed replications, three (10.7\%) stated their position in the aims and scope, whereas 25 (89.3\%) stated in within the detailed instructions to authors. The five-year impact factor (2015) of these journals ranged from 1.655 to 10.799, and nine of them (32.1\%) did not receive a 5-year or annual impact factor in 2015. There was no significant difference in the proportions of journals explicitly welcomed replications (journals with versus without impact factors, or high versus low impact factors). All sub-categories of neuroscience had at least a journal that welcomed replications. Discussion: The neuroscience journals that welcomed replications and published replications were reported. These pieces of information may provide descriptive information on the current journal practices regarding replication so the evidence-based recommendations to journal publishers can be made.},
	language = {English},
	urldate = {2019-08-03},
	journal = {Front. Hum. Neurosci.},
	author = {Yeung, Andy W. K.},
	year = {2017},
	keywords = {Information Science, JOURNAL EDITORIAL PRACTICES, Literature-Based Discovery, Neuroscience, Replication},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/FJ7JNVGL/Yeung - 2017 - Do Neuroscience Journals Accept Replications A Su.pdf:application/pdf}
}

@article{Assante_2016a,
	title = {Are {Scientific} {Data} {Repositories} {Coping} with {Research} {Data} {Publishing}?},
	volume = {15},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\textcopyright}, {\textregistered} or {\texttrademark} of their respective owners. No challenge to any owner{\textquoteright}s rights is intended or should be inferred.},
	issn = {1683-1470},
	url = {http://datascience.codata.org/articles/10.5334/dsj-2016-006/},
	doi = {10.5334/dsj-2016-006},
	abstract = {Research data publishing is intended as the release of research data to make it possible for practitioners to (re)use them according to {\textquotedblleft}open science{\textquotedblright} dynamics. There are three main actors called to deal with research data publishing practices: researchers, publishers, and data repositories. This study analyses the solutions offered by generalist scientific data repositories, i.e., repositories supporting the deposition of any type of research data. These repositories cannot make any assumption on the application domain. They are actually called to face with the almost open ended typologies of data used in science. The current practices promoted by such repositories are analysed with respect to eight key aspects of data publishing, i.e., dataset formatting, documentation, licensing, publication costs, validation, availability, discovery and access, and citation. From this analysis it emerges that these repositories implement well consolidated practices and pragmatic solutions for literature repositories. These practices and solutions can not totally meet the needs of management and use of datasets resources, especially in a context where rapid technological changes continuously open new exploitation prospects.},
	language = {eng},
	number = {0},
	urldate = {2019-08-03},
	journal = {Data Science Journal},
	author = {Assante, Massimiliano and Candela, Leonardo and Castelli, Donatella and Tani, Alice},
	month = apr,
	year = {2016},
	keywords = {Data infrastructures, Data Quality, Research Data Publishing, Scientific Data Repositories},
	pages = {6},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/JBAU9GDL/Assante et al. - 2016 - Are Scientific Data Repositories Coping with Resea.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/4M8ZCQ7P/dsj-2016-006.html:text/html}
}

@article{Candela_2015a,
	title = {Data journals: {A} survey},
	volume = {66},
	copyright = {{\textcopyright} 2015 ASIS\&T},
	issn = {2330-1643},
	shorttitle = {Data journals},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.23358},
	doi = {10.1002/asi.23358},
	abstract = {Data occupy a key role in our information society. However, although the amount of published data continues to grow and terms such as data deluge and big data today characterize numerous (research) initiatives, much work is still needed in the direction of publishing data in order to make them effectively discoverable, available, and reusable by others. Several barriers hinder data publishing, from lack of attribution and rewards, vague citation practices, and quality issues to a rather general lack of a data-sharing culture. Lately, data journals have overcome some of these barriers. In this study of more than 100 currently existing data journals, we describe the approaches they promote for data set description, availability, citation, quality, and open access. We close by identifying ways to expand and strengthen the data journals approach as a means to promote data set access and exploitation.},
	language = {en},
	number = {9},
	urldate = {2019-08-03},
	journal = {Journal of the Association for Information Science and Technology},
	author = {Candela, Leonardo and Castelli, Donatella and Manghi, Paolo and Tani, Alice},
	year = {2015},
	keywords = {alternative publications, data, publications},
	pages = {1747--1762},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/VY7Q7WPH/Candela et al. - 2015 - Data journals A survey.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/ADH2AX7R/asi.html:text/html}
}

@article{Vines_2013a,
	title = {Mandated data archiving greatly improves access to research data},
	volume = {27},
	issn = {0892-6638},
	url = {https://www.fasebj.org/doi/full/10.1096/fj.12-218164},
	doi = {10.1096/fj.12-218164},
	abstract = {The data underlying scientific papers should be accessible to researchers both now and in the future, but how best can we ensure that these data are available? Here we examine the effectiveness of four approaches to data archiving: no stated archiving policy, recommending (but not requiring) archiving, and two versions of mandating data deposition at acceptance. We control for differences between data types by trying to obtain data from papers that use a single, widespread population genetic analysis, structure. At one extreme, we found that mandated data archiving policies that require the inclusion of a data availability statement in the manuscript improve the odds of finding the data online almost 1000-fold compared to having no policy. However, archiving rates at journals with less stringent policies were only very slightly higher than those with no policy at all. We also assessed the effectiveness of asking for data directly from authors and obtained over half of the requested datasets, albeit with \~{}8 d delay and some disagreement with authors. Given the long-term benefits of data accessibility to the academic community, we believe that journal-based mandatory data archiving policies and mandatory data availability statements should be more widely adopted.{\textemdash}Vines, T. H., Andrew, R. L., Bock, D. G., Franklin, M. T., Gilbert, K. J., Kane, N. C., Moore, J-S., Moyers, B. T., Renaut, S., Rennison, D. J., Veen, T., Yeaman, S. Mandated data archiving greatly improves access to research data.},
	number = {4},
	urldate = {2019-08-03},
	journal = {The FASEB Journal},
	author = {Vines, Timothy H. and Andrew, Rose L. and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Kane, Nolan C. and Moore, Jean-S{\'e}bastien and Moyers, Brook T. and Renaut, S{\'e}bastien and Rennison, Diana J. and Veen, Thor and Yeaman, Sam},
	month = jan,
	year = {2013},
	pages = {1304--1308},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/8RA8G3NL/Vines et al. - 2013 - Mandated data archiving greatly improves access to.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/FNXZCS4W/fj.html:text/html}
}

@misc{Jomhari_2017,
	title = {Higgs-to-four-lepton analysis example using 2011-2012 data},
	url = {http://opendata.cern.ch/record/5500},
	urldate = {2019-08-03},
	publisher = {CERN Open Data Portal},
	author = {Jomhari, Nur Zulaiha and Geiser, Achim and Bin Anuar, Afiq Aizuddin},
	year = {2017},
	doi = {10.7483/opendata.cms.jkb8.rr42}
}

@article{Collaboration_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright {\textcopyright} 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that {\textquotedblleft}we already know this{\textquotedblright} belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2019-08-03},
	journal = {Science},
	author = {Collaboration, Open Science},
	month = aug,
	year = {2015},
	pmid = {26315443},
	pages = {aac4716},
	file = {Full Text PDF:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/HVVWAKPZ/Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf;Snapshot:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/VHPHTVVY/aac4716.html:text/html}
}

@article{Goodman_2016,
	title = {What does research reproducibility mean?},
	volume = {8},
	issn = {1946-6242},
	doi = {10.1126/scitranslmed.aaf5027},
	abstract = {The language and conceptual framework of "research reproducibility" are nonstandard and unsettled across the sciences. In this Perspective, we review an array of explicit and implicit definitions of reproducibility and related terminology, and discuss how to avoid potential misunderstandings when these terms are used as a surrogate for "truth."},
	language = {eng},
	number = {341},
	journal = {Sci Transl Med},
	author = {Goodman, Steven N. and Fanelli, Daniele and Ioannidis, John P. A.},
	year = {2016},
	pmid = {27252173},
	keywords = {Biomedical Research, Reproducibility of Results},
	pages = {341ps12},
	file = {Full Text:/home/julia/.mozilla/firefox/i2yyyfvo.default/zotero/storage/HUKD58L9/Goodman et al. - 2016 - What does research reproducibility mean.pdf:application/pdf}
}

@phdthesis{Sprenger_2014,
	title = {Spatial {Dependence} of the {Spike}-{Related} {Component} of the {Local} {Field} {Potential} in {Motor} {Cortex}},
	language = {English},
	author = {Sprenger, Julia},
	year = {2014},
	keywords = {Masterarbeit, Unver{\"o}ffentlichte Hochschulschrift}
}